{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This week, we will cover chapters 1 and 2 of the book:\n",
    "* The **first chapter** (*The Machine Learning Landscape*) is an introduction to the topic of machine learning\n",
    "* The **second chapter** (*End-to-End Machine Learning Project*) goes through an example machine learning project from end to end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two key concepts in the material of the second chapter which we will review:\n",
    "1. The design principles of `sklearn`\n",
    "2. The notion of pipelines in `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design principles of `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The principles behind `sklearn` are laid out in the paper [API design for machine learning software: experiences from the scikit-learn project](https://arxiv.org/abs/1309.0238)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any object that can estimate some parameters based on a dataset is called an **estimator** (e.g., an imputer is an estimator). The estimation itself is performed by the `fit()` method, and it takes only a dataset as a parameter (or two for supervised learning algorithms; the second dataset contains the labels). Any other parameter needed to guide the estimation process is considered a **hyperparameter** (such as an imputer ’s strategy), and it must be set as an instance variable (generally via a constructor parameter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some estimators (such as an imputer) can also transform a dataset; these are called **transformers**. Once again, the API is simple: the transformation is performed by the `transform()` method with the dataset to transform as a parameter. It returns the transformed dataset. This transformation generally relies on the learned parameters, as is the case for an imputer . All transformers also have a convenience method called `fit_transform()` that is equivalent to calling `fit()` and then `transform()` (but sometimes `fit_transform()` is optimized and runs much faster)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some estimators, given a dataset, are capable of making predictions; they are called **predictors**. A predictor has a `predict()` method that takes a dataset of new instances and returns a dataset of corresponding predictions. It also has a `score()` method that measures the quality of the predictions, given a test set (and the corresponding labels, in the case of supervised learning algorithms)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the estimator’s **hyperparameters** are accessible directly via public instance variables (e.g., `imputer.strategy`), and all the estimator’s **learned parameters** are accessible via public instance variables with an underscore suffix (e.g. `imputer.statistics_`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonproliferation of classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets are represented as `NumPy` arrays or `SciPy` sparse matrices, instead of homemade classes. Hyperparameters are just regular Python strings or numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Composition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existing building blocks are reused as much as possible. For example, it is easy to create a `Pipeline` estimator from an arbitrary sequence of transformers followed by a final estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensible defaults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn provides reasonable default values for most parameters, making it easy to quickly create a baseline working system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines in sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipelines are best-practice to encapsulate the entire data processing that is required to get from the raw data (as read in from a file or database table) to the inputs of a model. Pipelines chain multiple transformers that each perform specific tasks. The last element of a pipeline can be an estimator. In this case, the estimator combines data processing and model estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main challenge with pipelines is that sklearn and pandas do not interact seamlessly all the time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Building a regression model for the Titanic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
    "col_types = {'Sex': 'category', 'Embarked': 'category'}\n",
    "train = pd.read_csv('data/train.csv', usecols=cols, dtype=col_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 8 columns):\n",
      "Survived    891 non-null int64\n",
      "Pclass      891 non-null int64\n",
      "Sex         891 non-null category\n",
      "Age         714 non-null float64\n",
      "SibSp       891 non-null int64\n",
      "Parch       891 non-null int64\n",
      "Fare        891 non-null float64\n",
      "Embarked    889 non-null category\n",
      "dtypes: category(2), float64(2), int64(4)\n",
      "memory usage: 43.8 KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass     Sex   Age  SibSp  Parch     Fare Embarked\n",
       "0         0       3    male  22.0      1      0   7.2500        S\n",
       "1         1       1  female  38.0      1      0  71.2833        C\n",
       "2         1       3  female  26.0      0      0   7.9250        S\n",
       "3         1       1  female  35.0      1      0  53.1000        S\n",
       "4         0       3    male  35.0      0      0   8.0500        S"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train['Survived'].astype('float32').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop('Survived', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to get this `DataFrame` into a shape that `sklearn` estimators can work with, i.e. a `numpy` array with dtype float. We will do this with a pipeline transformer that contains all necessary steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers: Building blocks of pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each class you intent to put in a pipeline or feature union should inherit from `BaseEstimator` and `TransformerMixin`.\n",
    "* The former makes your class accessible for hyper parameter methods such as `GridSearch`\n",
    "* The latter applies a robust fit_transform function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the base classes the only mandatory functions are `fit` and `transform`. Additional useful class methods for transformers are:\n",
    "* With `__init__` you can give your transformer some config at initialization\n",
    "* The `inverse__transform` enables the inverse transform call on the full pipeline if each element supports it. \n",
    "* The `get_feature_names` function is crucial when you want the names of transformed features in a feature union accessible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of a transformer that applies a vectorized function `trans_func` and inverse function `untrans_func` to the attributes specified in `columns` in a `DataFrame` could look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Apply given transformation.\"\"\"\n",
    "    def __init__(self, trans_func, untrans_func, columns):\n",
    "        self.transform_func = trans_func\n",
    "        self.inverse_transform_func = untrans_func\n",
    "        self.cols = columns\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, x):\n",
    "        x = self._get_selection(x)\n",
    "        return self.transform_func(x) if callable(self.transform_func) else x\n",
    "\n",
    "    def inverse_transform(self, x):\n",
    "        return self.inverse_transform_func(x) \\\n",
    "            if callable(self.inverse_transform_func) else x\n",
    "\n",
    "    def _get_selection(self, df):\n",
    "        assert isinstance(df, pd.DataFrame), \"df is not a pandas DataFrame\"\n",
    "        return df[self.cols]\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        return self.cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to the example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sklearn` provides the `ColumnTransformer` class which selects columns by name. The selected subset of columns can then be further processed. Typically, columns with the same type need to be processed in the same manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A possible implementation of a `TypeSelector` is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TypeSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, dtype):\n",
    "        self.dtype = dtype\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        assert isinstance(X, pd.DataFrame), \"X is not a DataFrame\"\n",
    "        return X.select_dtypes(include=[self.dtype])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the data, we need two pipelines\n",
    "1. Categorical data: Categories need to be encoded.\n",
    "2. Numeric data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Categorical data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recently, `pandas` has introduced the `categorical` data type which handles most of the work related to categorical data. However, categoricals are not accepted by most machine learning models and require further encoding or creating dummy variables first. In addition, `pandas` `categorical` type does not include `NaN` values in the list of values and replaces them with -1. Many encoder classes in `sklearn`, including `OneHotEncoder` expect positive values. This mismatch between the two packages needs to be explicitly addressed:\n",
    "1. Make the missing values an explicit category by changing the index\n",
    "2. Imputation of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MissingCategory(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        assert isinstance(X, pd.DataFrame), \"X is not a DataFrame\"\n",
    "        assert X.apply(lambda c: c.dtype).eq('category').all(), \"Not all columns of X have category dtype\"\n",
    "        return X.apply(lambda s: s.cat.codes.replace(\n",
    "            {-1: len(s.cat.categories)}\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_cat = Pipeline([\n",
    "    ('selector', TypeSelector('category')),\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cat = pipeline_cat.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. 0. 1.]\n",
      " [1. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(X_cat[:4,].todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Numeric data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For numeric data, we need to distingish between integer values data (which are really categorical variables) and numeric data. For integer data, we need to apply the same pipeline as for categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_int = Pipeline([\n",
    "    ('selector', TypeSelector(int)),\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(categories='auto'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "X_int = pipeline_int.fit_transform(X)\n",
    "print(X_int[:4,].todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For true numeric data, the steps are imputation and then scaling as many ML algorithms require that features have a similar value range. We will use standardization here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_num = Pipeline([\n",
    "    ('selector', TypeSelector('float')),\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.56573646 -0.50244517]\n",
      " [ 0.66386103  0.78684529]\n",
      " [-0.25833709 -0.48885426]\n",
      " [ 0.4333115   0.42073024]]\n"
     ]
    }
   ],
   "source": [
    "X_num = pipeline_num.fit_transform(X)\n",
    "print(X_num[:4,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combining multiple parallel pipelines**: FeatureUnion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `FeatureUnion` class concatenates the results of multiple pipelines into a single numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pipeline = FeatureUnion(transformer_list=[\n",
    "        (\"numeric_features\", pipeline_num),\n",
    "        (\"categorical_features\", pipeline_cat),\n",
    "        (\"integer_features\", pipeline_int)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.56573646 -0.50244517  0.          1.          0.          0.\n",
      "   1.          0.          0.          1.          0.          1.\n",
      "   0.          0.          0.          0.          0.          1.\n",
      "   0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.66386103  0.78684529  1.          0.          1.          0.\n",
      "   0.          1.          0.          0.          0.          1.\n",
      "   0.          0.          0.          0.          0.          1.\n",
      "   0.          0.          0.          0.          0.          0.        ]\n",
      " [-0.25833709 -0.48885426  1.          0.          0.          0.\n",
      "   1.          0.          0.          1.          1.          0.\n",
      "   0.          0.          0.          0.          0.          1.\n",
      "   0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.4333115   0.42073024  1.          0.          0.          0.\n",
      "   1.          1.          0.          0.          0.          1.\n",
      "   0.          0.          0.          0.          0.          1.\n",
      "   0.          0.          0.          0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "X_processed = full_pipeline.fit_transform(X)\n",
    "print(X_processed[:4,].todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make a model pipeline**: Add an estimator as the final pipeline step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When expanding a dataset it is often required to keep the actual dataset in the feature union, which is not retained by default. We can work around this by having an identity transformer. We can even use our new class SimpleTransformer for that - just pass None for trans_func and untrans_func."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very basic feature union that expects the pandas dataframe format can look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_feature_names = ['Age', 'Gender', 'Height', 'Weight', 'y1', 'y2']\n",
    "\n",
    "simple_union = FeatureUnion([('simple_trans_y',\n",
    "                               SimpleTransformer(np.sqrt, np.square,\n",
    "                                                 ['y1', 'y2'])\n",
    "                              ),\n",
    "                              ('identity',\n",
    "                               SimpleTransformer(None, None,\n",
    "                                ['Age', 'Gender', 'Height', 'Weight'])\n",
    "                              )\n",
    "                             ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The FeatureUnion class just takes as argument a list of tuples where each tuple consists of a name and a transformer. Here, a dataset is simply filtered by subsets of all_feature_names where the columns y1 and y2 are transformed into their square root (assume their content to be floating point numbers). Well, the output of this union will still be a numpy matrix, but hold on, the magic is still to come. So far we have seen how to have transformations specific to certain columns but the df format is not retained yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling your dataset by a StandardScaler or MinMaxScaler is what data scientists do for a living since a lot of linear models rely on this preprocessing in order to learn the latent patterns. But what if we want only specific columns to be scaled? What if I need a separate scaling for my independent and dependent features since I want to inverse the scaling of my predictions later that naturally embrace only the target variables?Scaling your dataset by a StandardScaler or MinMaxScaler is what data scientists do for a living since a lot of linear models rely on this preprocessing in order to learn the latent patterns. But what if we want only specific columns to be scaled? What if I need a separate scaling for my independent and dependent features since I want to inverse the scaling of my predictions later that naturally embrace only the target variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scaler(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"scales selected columns only with given scaler\"\"\"\n",
    "    def __init__(self, scaler, columns):\n",
    "        self.scaler = scaler\n",
    "        self.cols = columns\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = self._get_selection(X)\n",
    "        self.scaler.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = self._get_selection(X)\n",
    "        return self.scaler.transform(X)\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        return self.scaler.inverse_transform(X)\n",
    "\n",
    "    def _get_selection(self, df):\n",
    "        assert isinstance(df, pd.DataFrame)\n",
    "        return df[self.cols]\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        return self.cols\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This scaler class is pretty straight forward and calls in each of its functions the corresponding scaler function, where the scaler was given during initialization. The difference to the SimpleTransformer is that we now also do something during fitting but no surprises here. Build the feature union like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "scaling_union = FeatureUnion([('scaler_x',\n",
    "                              Scaler(StandardScaler(),\n",
    "                                     ['Age', 'Gender', 'Height', 'Weight']),\n",
    "                              ('scaler_y',\n",
    "                               Scaler(StandardScaler(),\n",
    "                                      ['y1', 'y2']))\n",
    "                             ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rolling Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rolling transformations are common when working with Time Series Data, and are easily implemented by relying on pandas' functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RollingFeatures(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"This Transformer adds rolling statistics\"\"\"\n",
    "    def __init__(self, columns, lookback=10):\n",
    "        self.lookback = lookback\n",
    "        self.cols = columns\n",
    "        self.transformed_cols = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = self._get_selection(X)\n",
    "        feat_d = {'std': X.rolling(self.lookback).std(),\n",
    "                  'mean': X.rolling(self.lookback).mean(),\n",
    "                  'sum': X.rolling(self.lookback).sum()\n",
    "                  }\n",
    "        for k in feat_d:\n",
    "            feat_d[k].columns = \\\n",
    "                ['{}_rolling{}_{}'.format(c, self.lookback, k) for\n",
    "                 c in X.columns]\n",
    "        df = pd.concat(list(feat_d.values()), axis=1)\n",
    "        self.transformed_cols = list(df.columns)\n",
    "        return df\n",
    "\n",
    "    def _get_selection(self, df):\n",
    "        assert isinstance(df, pd.DataFrame)\n",
    "        return df[self.cols]\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        return self.transformed_cols\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we come up with completely new feature names the first time. Note, that we have a new class variable transformed_cols to take account of those cols that were generated here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A class to perform some simple cleaning of a `DataFrame` could look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BaseEstimator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-db5f2195c0e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mDFCleaner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseEstimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTransformerMixin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BaseEstimator' is not defined"
     ]
    }
   ],
   "source": [
    "class DFCleaner(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        assert isinstance(X, pd.DataFrame)\n",
    "        X.dropna(inplace=True)\n",
    "        X.reset_index(drop=True, inplace=True)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retain DataFrame as output format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this challenge we can exploit the following simple trick. The FeatureUnion class has a method called get_feature_names that exhibits the feature names of each transformer although their output is a numpy matrix. In order to workaround the numpy output we can make each feature union a two-step pipeline where the union denotes the first step while a transformer fetching the actual feature names represents the second step. Sounds crazy? Check this out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureUnionReframer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Transforms preceding FeatureUnion's output back into Dataframe\"\"\"\n",
    "    def __init__(self, feat_union, cutoff_transformer_name=True):\n",
    "        self.union = feat_union\n",
    "        self.cutoff_transformer_name = cutoff_transformer_name\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        assert isinstance(X, np.ndarray)\n",
    "        if self.cutoff_transformer_name:\n",
    "            cols = [c.split('__')[1] for c in self.union.get_feature_names()]\n",
    "        else:\n",
    "            cols = self.union.get_feature_names()\n",
    "        df = pd.DataFrame(data=X, columns=cols)\n",
    "        return df\n",
    "\n",
    "    @classmethod\n",
    "    def make_df_retaining(cls, feature_union):\n",
    "        \"\"\"With this method a feature union will be returned as a pipeline\n",
    "        where the first step is the union and the second is a transformer that\n",
    "        re-applies the columns to the union's output\"\"\"\n",
    "        return Pipeline([('union', feature_union),\n",
    "                         ('reframe', cls(feature_union))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class does the job. The optional bool argument gives you the freedom to keep the default prefix in feature union chains that is the name of the transformer. The class’ static method is making things even more comfortable as we don’t need to instantiate it explicitly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_union, make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "x_feats = ['Age', 'Gender', 'Height', 'Weight']\n",
    "y_feats = ['y1', 'y2']\n",
    "\n",
    "featurize_union = make_union(SimpleTransformer(np.sqrt, np.square, y_feats),\n",
    "                             SimpleTransformer(None, None, x_feats),\n",
    "                             RollingFeatures(x_feats, lookback=10)\n",
    "                             )\n",
    "scaling_union = make_union(Scaler(StandardScaler(), x_feats),\n",
    "                           Scaler(StandardScaler(), y_feats)\n",
    "                          )\n",
    "\n",
    "featurize_pipe = FeatureUnionReframer.make_df_retaining(featurize_union)\n",
    "scaling_pipe = FeatureUnionReframer.make_df_retaining(scaling_union)\n",
    "\n",
    "pipe = make_pipeline(featurize_pipe,\n",
    "                     DFCleaner(),\n",
    "                     scaling_pipe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build an example pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every blue segment is a standard scikit-learn `Transformer`. The yellow segments are custom-made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ColumnSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TypeSelector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often data is loaded in a pandas `DataFrame` which can combine several types of variables. Sklearn, however, works on numpy arrays with a floating point data type  (the default is `float64` but `float32` can also be specified. Therefore, most pipelines perform a type selection as their first step and all variables of the same type are processed in the same manner. If necessary, of course, additional selection steps can be performed so that specific groups of variables are processed differently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sklearn` does not provide a type selector class out of the box, presumably as sklearn is built around `numpy` and the \"problem\" is introduced by the pandas package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catagory Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only problem that needs to be explicitly addressed is that the `pandas` `categorical` type does not include `NaN` values in the list of values and replaces them with -1. Many encoder classes in `sklearn`, including `OneHotEncoder` expect positive values. This mismatch between the two packages needs to be explicitly addressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Pipeline([\n",
    "    ('features', FeatureUnion(n_jobs=1, transformer_list=[\n",
    "        # Part 1\n",
    "        ('boolean', Pipeline([\n",
    "            ('selector', TypeSelector('bool')),\n",
    "        ])),  # booleans close\n",
    "        \n",
    "        ('numericals', Pipeline([\n",
    "            ('selector', TypeSelector(np.number)),\n",
    "            ('scaler', StandardScaler()),\n",
    "        ])),  # numericals close\n",
    "        \n",
    "        # Part 2\n",
    "        ('categoricals', Pipeline([\n",
    "            ('selector', TypeSelector('category')),\n",
    "            ('labeler', StringIndexer()),\n",
    "            ('encoder', OneHotEncoder(handle_unknown='ignore')),\n",
    "        ]))  # categoricals close\n",
    "    ])),  # features close\n",
    "])  # pipeline close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, we will be using the"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation without a Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pmlb'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-64c05fdc7fb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpmlb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpmlb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'churn'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_X_y\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pmlb'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pmlb\n",
    "\n",
    "df = pmlb.fetch_data('churn', return_X_y=False)\n",
    "\n",
    "# Remove the target column and the phone number\n",
    "x_cols = [c for c in df if c not in [\"target\", \"phone number\"]]\n",
    "\n",
    "binary_features = [\"international plan\", \"voice mail plan\"]\n",
    "categorical_features = [\"state\", \"area code\"]\n",
    "\n",
    "# Column types are defaulted to floats\n",
    "X = (\n",
    "    df\n",
    "    .drop([\"target\"], axis=1)\n",
    "    .astype(float)\n",
    ")\n",
    "X[binary_features] = X[binary_features].astype(\"bool\")\n",
    "\n",
    "# Categorical features can't be set all at once\n",
    "for f in categorical_features:\n",
    "    X[f] = X[f].astype(\"category\")\n",
    "\n",
    "y = df.target\n",
    "\n",
    "# Randomly set 500 items as missing values\n",
    "random.seed(42)\n",
    "num_missing = 500\n",
    "indices = [(row, col) for row in range(X.shape[0]) for col in range(X.shape[1])]\n",
    "for row, col in random.sample(indices, num_missing):\n",
    "    X.iat[row, col] = np.nan\n",
    "\n",
    "# Partition data set into training/test split (2 to 1 ratio)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/3., random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        assert isinstance(X, pd.DataFrame)\n",
    "\n",
    "        try:\n",
    "            return X[self.columns]\n",
    "        except KeyError:\n",
    "            cols_error = list(set(self.columns) - set(X.columns))\n",
    "            raise KeyError(\"The DataFrame does not include the columns: %s\" % cols_error)\n",
    "            \n",
    "cs = ColumnSelector(columns=[\"state\", \"account length\", \"area code\"])\n",
    "cs.fit_transform(df).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TypeSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, dtype):\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        assert isinstance(X, pd.DataFrame)\n",
    "        return X.select_dtypes(include=[self.dtype])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = TypeSelector(\"category\")\n",
    "ts.fit_transform(X).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Select only the relevant feature columns (omits the phone number column)\n",
    "2. Impute and standardize the numeric features\n",
    "3. Impute and one-hot encode the categorical features\n",
    "4. Impute the boolean features\n",
    "5. Apply a FeatureUnion to join the transformed features into a single data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_pipeline = make_pipeline(\n",
    "    ColumnSelector(columns=x_cols),\n",
    "    FeatureUnion(transformer_list=[\n",
    "        (\"numeric_features\", make_pipeline(\n",
    "            TypeSelector(np.number),\n",
    "            Imputer(strategy=\"median\"),\n",
    "            StandardScaler()\n",
    "        )),\n",
    "        (\"categorical_features\", make_pipeline(\n",
    "            TypeSelector(\"category\"),\n",
    "            Imputer(strategy=\"most_frequent\"),\n",
    "            OneHotEncoder()\n",
    "        )),\n",
    "        (\"boolean_features\", make_pipeline(\n",
    "            TypeSelector(\"bool\"),\n",
    "            Imputer(strategy=\"most_frequent\")\n",
    "        ))\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_pipeline.fit(X_train)\n",
    "\n",
    "X_test_transformed = preprocess_pipeline.transform(X_test)\n",
    "X_test_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_pipeline = make_pipeline(\n",
    "    preprocess_pipeline,\n",
    "    SVC(kernel=\"rbf\", random_state=42)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"svc__gamma\": [0.1 * x for x in range(1, 6)]\n",
    "}\n",
    "\n",
    "classifier_model = GridSearchCV(classifier_pipeline, param_grid, cv=10)\n",
    "classifier_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score = classifier_model.decision_function(X_test)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_score)\n",
    "roc_auc = roc_auc_score(y_test, y_score)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(16, 12))\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)', size=16)\n",
    "plt.ylabel('True Positive Rate (Sensitivity)', size=16)\n",
    "plt.title('ROC Curve', size=20)\n",
    "plt.legend(fontsize=14);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
