{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the fundamental idea behind Support Vector Machines?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the widest margin, either between classes (classification) or within the data (regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is a support vector?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A support vector determines a hyperplane. The support vector is orthogonal to the hyperplance and points to the upperside of the plane. A hyperplane is an n-1 dimensional subspace in an n-dimensional space. It cuts the space in two halves.\n",
    "\n",
    "In an SVM, a support vector is any data point within the margin around the seperating hyperplance between classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is it important to scale the inputs when using SVMs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If features are not scaled, larger feature will become more important to the determination of support vectors as their deveations are larger while smaller features will be neglected.\n",
    "\n",
    "For the implementation in Scikit-learn, features need to be centered as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can an SVM classifier output a confidence score when it classifies an instance? What about a probability?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM is a discriminative method, and does not calculate probabilities or confidence scores. The distance to the hyperplane can be reported, which acts like a confidence score.\n",
    "\n",
    "Scikit-learn fits a logistic-regression on the predicted classes of the SVM to determine class probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should you use the primal or the dual form of the SVM problem to train a model on a training set with millions of instances and hundreds of features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For linear kernel, if the number of features is less than the number of instances, the primal problem should be solved. If the number of instances is larger than the number of features, solving the dual problem is more efficient.\n",
    "\n",
    "In this case, the primal problem should be solved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say you’ve trained an SVM classifier with an RBF kernel, but it seems to underfit the training set. Should you increase or decrease γ ( gamma )? What about C ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Underfitting suggests that the model requires more degrees of freedom to capture the problem. Gamma should be increased such that the decision boundary becomes more \"wiggly\". C should be increased as well, to place more weight on the slack variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How should you set the QP parameters (H, f, A, and b) to solve the soft margin linear SVM classifier problem using an off-the-shelf QP solver?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a LinearSVC on a linearly separable dataset. Then train an SVC and a SGDClassifier on the same dataset. See if you can get them to produce roughly the same model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train an SVM classifier on the MNIST dataset. Since SVM classifiers are binary classifiers, you will need to use one-versus-the-rest to classify all 10 digits. You may want to tune the hyperparameters using small validation sets to speed up the process. What accuracy can you reach?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import cpu_count\n",
    "n_jobs = cpu_count() - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "X = (X/256.).astype(np.float32)\n",
    "y = y.astype(np.uint8)\n",
    "\n",
    "X_train = X[:60000]\n",
    "y_train = y[:60000]\n",
    "X_test = X[60000:]\n",
    "y_test = y[60000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try first a linear SVC kernel, to establish **baseline performance**. Solve dual problem if n_samples <= n_features, otherwise solve primal problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples, n_features = X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = make_pipeline(StandardScaler(), LinearSVC(dual = n_samples <= n_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/philipp/miniconda3/envs/tf2/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('standardscaler',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('linearsvc',\n",
       "                 LinearSVC(C=1.0, class_weight=None, dual=False,\n",
       "                           fit_intercept=True, intercept_scaling=1,\n",
       "                           loss='squared_hinge', max_iter=1000,\n",
       "                           multi_class='ovr', penalty='l2', random_state=None,\n",
       "                           tol=0.0001, verbose=0))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9274666666666667"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:   0.93\n",
      "Test Accuracy:   0.92\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_train)\n",
    "y_pred_test = clf.predict(X_test)\n",
    "print('Train Accuracy: {:6.2f}'.format(accuracy_score(y_train, y_pred)))\n",
    "print('Test Accuracy: {:6.2f}'.format(accuracy_score(y_test, y_pred_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build pipeline over multiple approaches to find best approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_model = Pipeline([('scaler',StandardScaler()), ('clf',KNeighborsClassifier())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {'clf':[KNeighborsClassifier()], 'clf__weights': [\"uniform\", \"distance\"], 'clf__n_neighbors': [3, 4, 5]},\n",
    "    {'clf':[SVC()], 'clf__kernel':['linear', 'rbf'], 'clf__C': [1., 10., 100., 1000.], 'clf__gamma':[0.01, 0.1, 1.]},\n",
    "    {'clf':[RandomForestClassifier()], 'clf__n_estimators': [10, 100, 1000], 'clf__max_depth':[None, 5, 10]},\n",
    "    {'clf':[GradientBoostingClassifier()], 'clf__n_estimators': [10, 100, 1000], 'clf__learning_rate':[0.01, 0.1, 0.25]},\n",
    "    {'clf':[MLPClassifier(early_stopping=True)], 'clf__hidden_layer_sizes':[(16,), (16,16,), (32,16,),(32,32,)], 'clf__alpha':[0.000001, 0.00001, 0.0001], 'clf__learning_rate':['invscaling', 'adaptive']}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(pipeline_model, param_grid, scoring=\"accuracy\", cv=3, n_jobs=n_jobs, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 72 candidates, totalling 216 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=11)]: Using backend LokyBackend with 11 concurrent workers.\n",
      "[Parallel(n_jobs=11)]: Done  19 tasks      | elapsed: 59.4min\n",
      "/home/philipp/miniconda3/envs/tf2/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=11)]: Done 140 tasks      | elapsed: 1521.4min\n",
      "[Parallel(n_jobs=11)]: Done 216 out of 216 | elapsed: 1689.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('scaler',\n",
       "                                        StandardScaler(copy=True,\n",
       "                                                       with_mean=True,\n",
       "                                                       with_std=True)),\n",
       "                                       ('clf',\n",
       "                                        KNeighborsClassifier(algorithm='auto',\n",
       "                                                             leaf_size=30,\n",
       "                                                             metric='minkowski',\n",
       "                                                             metric_params=None,\n",
       "                                                             n_jobs=None,\n",
       "                                                             n_neighbors=5, p=2,\n",
       "                                                             weights='uniform'))],\n",
       "                                verbose=False),\n",
       "             iid='deprecated', n_jobs=11,\n",
       "             param_grid=[{'clf': [KNe...\n",
       "                                                power_t=0.5, random_state=None,\n",
       "                                                shuffle=True, solver='adam',\n",
       "                                                tol=0.0001,\n",
       "                                                validation_fraction=0.1,\n",
       "                                                verbose=False,\n",
       "                                                warm_start=False)],\n",
       "                          'clf__alpha': [1e-06, 1e-05, 0.0001],\n",
       "                          'clf__hidden_layer_sizes': [(16,), (16, 16), (32, 16),\n",
       "                                                      (32, 32)],\n",
       "                          'clf__learning_rate': ['invscaling', 'adaptive']}],\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='accuracy', verbose=2)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['grid.pkl']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(grid, 'grid.pkl', compress=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('grid.pkl', 'rb') as f:\n",
    "    grid = joblib.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('scaler',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('clf',\n",
       "                 GradientBoostingClassifier(ccp_alpha=0.0,\n",
       "                                            criterion='friedman_mse', init=None,\n",
       "                                            learning_rate=0.1, loss='deviance',\n",
       "                                            max_depth=3, max_features=None,\n",
       "                                            max_leaf_nodes=None,\n",
       "                                            min_impurity_decrease=0.0,\n",
       "                                            min_impurity_split=None,\n",
       "                                            min_samples_leaf=1,\n",
       "                                            min_samples_split=2,\n",
       "                                            min_weight_fraction_leaf=0.0,\n",
       "                                            n_estimators=1000,\n",
       "                                            n_iter_no_change=None,\n",
       "                                            presort='deprecated',\n",
       "                                            random_state=None, subsample=1.0,\n",
       "                                            tol=0.0001, validation_fraction=0.1,\n",
       "                                            verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:   1.00\n",
      "Test Accuracy: 0.977299999999999946531659\n"
     ]
    }
   ],
   "source": [
    "y_pred = grid.predict(X_train)\n",
    "y_pred_test = grid.predict(X_test)\n",
    "print('Train Accuracy: {:6.2f}'.format(accuracy_score(y_train, y_pred)))\n",
    "print('Test Accuracy: {:6.24f}'.format(accuracy_score(y_test, y_pred_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 (tf2)",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
