{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Chapter 11 - Training Deep Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Is it OK to initialize all the weights to the same value as long as that value is selected randomly using He initialization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "If all weights are initialized with the same value and BackProp via Gradient Descent is used to train a network, then weights will always stay the same. It is highly unlikely that a good solution could be found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Is it OK to initialize the bias terms to 0?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Yes, which is also commonly done. The initalization of the bias is not very important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Name three advantages of the SELU activation function over ReLU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "1. SELU is continuously differentiable as opposed to ReLU, where the gradient can jump and learning oscillates.\n",
    "2. SELU does not run the risk of dead nodes.\n",
    "3. SELU produces on average values closer to zero as it can return negative values. This stabilizes the variance of different layers and helps with vanishing or exploding gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Problem 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "In which cases would you want to use each of the following activation functions: SELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "* tanh, logistic, and softmax are primarily useful for the output layer. tanh if the output range is bounded, logistic for binary classification of bounded ranges, softmax for multiclass classification.\n",
    "* ReLU is preferrable if prediction speed is critical\n",
    "* Leaky ReLU allows for learning everywhere, and prevents dead nodes, and is faster than SELU.\n",
    "* SELU tends to perform best, but requires more compute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Problem 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using an SGD optimizer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "When momentum is very high, the step direction is updated very slowly. This risks that the parameter search overshoots and takes a long time to correct itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Problem 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Name three ways you can produce a sparse model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "1. L1 regularization\n",
    "2. Pruning, i.e. set sufficiently small weights to zero.\n",
    "3. Use the TensorFlow Model Optimization Toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Problem 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Does dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)? What about MC Dropout?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "1. Yes, dropout slows down training - in general - as in each training step any given node may or may not learn\n",
    "2. Dropout does not slow down inference, as all nodes are always included.\n",
    "3. MC Dropout slows down inference, as multiple predictions need to be made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Problem 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Practice training a deep neural network on the CIFAR10 image dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, test_size=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Problem 8a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Build a DNN with 20 hidden layers of 100 neurons each (that’s too many, but it’s the point of this exercise). Use He initialization and the ELU activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=X_train.shape[1:]))\n",
    "for i in range(20):\n",
    "    model.add(keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Problem 8b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Using Nadam optimization and early stopping, train the network on the\n",
    "CIFAR10 dataset. You can load it with keras.datasets.cifar10.load_\n",
    "data() . The dataset is composed of 60,000 32 × 32–pixel color images (50,000 for training, 10,000 for testing) with 10 classes, so you’ll need a softmax output layer with 10 neurons. Remember to search for the right learning rate each time you change the model’s architecture or hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "model.add(keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_cifar10_model.h5\", save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-7c5f445082a5dfab\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-7c5f445082a5dfab\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir=./my_cifar10_logs --port=6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/25\n",
      "45000/45000 [==============================] - 14s 310us/sample - loss: 13.8890 - accuracy: 0.1843 - val_loss: 2.0346 - val_accuracy: 0.2118\n",
      "Epoch 2/25\n",
      "45000/45000 [==============================] - 11s 243us/sample - loss: 469092825.2416 - accuracy: 0.1806 - val_loss: 2.3073 - val_accuracy: 0.0936\n",
      "Epoch 3/25\n",
      "45000/45000 [==============================] - 11s 246us/sample - loss: 2.3054 - accuracy: 0.1004 - val_loss: 2.3027 - val_accuracy: 0.1012\n",
      "Epoch 4/25\n",
      "45000/45000 [==============================] - 11s 252us/sample - loss: 2.3062 - accuracy: 0.1000 - val_loss: 2.3119 - val_accuracy: 0.0958\n",
      "Epoch 5/25\n",
      "45000/45000 [==============================] - 11s 253us/sample - loss: 2.3066 - accuracy: 0.0992 - val_loss: 2.3127 - val_accuracy: 0.0936\n",
      "Epoch 6/25\n",
      "45000/45000 [==============================] - 11s 237us/sample - loss: 2.3074 - accuracy: 0.1002 - val_loss: 2.3131 - val_accuracy: 0.1062\n",
      "Epoch 7/25\n",
      "45000/45000 [==============================] - 11s 240us/sample - loss: 2.3082 - accuracy: 0.0984 - val_loss: 2.3042 - val_accuracy: 0.0998\n",
      "Epoch 8/25\n",
      "45000/45000 [==============================] - 11s 247us/sample - loss: 2.3083 - accuracy: 0.0988 - val_loss: 2.3078 - val_accuracy: 0.1012\n",
      "Epoch 9/25\n",
      "45000/45000 [==============================] - 11s 250us/sample - loss: 2.3081 - accuracy: 0.0991 - val_loss: 2.3123 - val_accuracy: 0.0998\n",
      "Epoch 10/25\n",
      "45000/45000 [==============================] - 11s 253us/sample - loss: 2.3091 - accuracy: 0.0979 - val_loss: 2.3166 - val_accuracy: 0.0998\n",
      "Epoch 11/25\n",
      "45000/45000 [==============================] - 11s 244us/sample - loss: 2.3089 - accuracy: 0.0983 - val_loss: 2.3141 - val_accuracy: 0.0936\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/25\n",
      "45000/45000 [==============================] - 13s 294us/sample - loss: 5.3554 - accuracy: 0.2225 - val_loss: 2.0232 - val_accuracy: 0.2554\n",
      "Epoch 2/25\n",
      "45000/45000 [==============================] - 11s 245us/sample - loss: 1.9384 - accuracy: 0.2838 - val_loss: 1.9713 - val_accuracy: 0.2918\n",
      "Epoch 3/25\n",
      "45000/45000 [==============================] - 11s 242us/sample - loss: 1.8903 - accuracy: 0.3064 - val_loss: 1.9601 - val_accuracy: 0.2806\n",
      "Epoch 4/25\n",
      "45000/45000 [==============================] - 11s 247us/sample - loss: 1.8926 - accuracy: 0.3074 - val_loss: 1.9083 - val_accuracy: 0.3140\n",
      "Epoch 5/25\n",
      "45000/45000 [==============================] - 12s 257us/sample - loss: 1.8624 - accuracy: 0.3214 - val_loss: 1.8715 - val_accuracy: 0.3340\n",
      "Epoch 6/25\n",
      "45000/45000 [==============================] - 11s 251us/sample - loss: 1.8584 - accuracy: 0.3241 - val_loss: 1.9613 - val_accuracy: 0.3186\n",
      "Epoch 7/25\n",
      "45000/45000 [==============================] - 12s 259us/sample - loss: 1.8851 - accuracy: 0.3104 - val_loss: 2.2443 - val_accuracy: 0.2068\n",
      "Epoch 8/25\n",
      "45000/45000 [==============================] - 12s 262us/sample - loss: 3.2900 - accuracy: 0.2578 - val_loss: 2.3061 - val_accuracy: 0.1000\n",
      "Epoch 9/25\n",
      "45000/45000 [==============================] - 11s 247us/sample - loss: 2.3170 - accuracy: 0.1013 - val_loss: 2.3178 - val_accuracy: 0.1024\n",
      "Epoch 10/25\n",
      "45000/45000 [==============================] - 11s 249us/sample - loss: 2.3193 - accuracy: 0.1030 - val_loss: 2.3244 - val_accuracy: 0.1000\n",
      "Epoch 11/25\n",
      "45000/45000 [==============================] - 11s 248us/sample - loss: 2.3213 - accuracy: 0.1006 - val_loss: 2.3074 - val_accuracy: 0.1012\n",
      "Epoch 12/25\n",
      "45000/45000 [==============================] - 11s 256us/sample - loss: 2.3246 - accuracy: 0.1011 - val_loss: 2.3467 - val_accuracy: 0.0958\n",
      "Epoch 13/25\n",
      "45000/45000 [==============================] - 11s 248us/sample - loss: 2.3275 - accuracy: 0.1006 - val_loss: 2.3529 - val_accuracy: 0.0936\n",
      "Epoch 14/25\n",
      "45000/45000 [==============================] - 12s 262us/sample - loss: 2.3298 - accuracy: 0.1006 - val_loss: 2.3217 - val_accuracy: 0.1002\n",
      "Epoch 15/25\n",
      "45000/45000 [==============================] - 11s 250us/sample - loss: 2.3310 - accuracy: 0.1007 - val_loss: 2.3259 - val_accuracy: 0.1002\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/25\n",
      "45000/45000 [==============================] - 13s 283us/sample - loss: 3.4933 - accuracy: 0.2012 - val_loss: 2.1036 - val_accuracy: 0.2004\n",
      "Epoch 2/25\n",
      "45000/45000 [==============================] - 11s 240us/sample - loss: 1.9311 - accuracy: 0.2924 - val_loss: 1.9058 - val_accuracy: 0.3102\n",
      "Epoch 3/25\n",
      "45000/45000 [==============================] - 11s 246us/sample - loss: 1.8689 - accuracy: 0.3172 - val_loss: 1.8668 - val_accuracy: 0.3174\n",
      "Epoch 4/25\n",
      "45000/45000 [==============================] - 11s 252us/sample - loss: 1.8419 - accuracy: 0.3286 - val_loss: 1.8061 - val_accuracy: 0.3450\n",
      "Epoch 5/25\n",
      "45000/45000 [==============================] - 11s 249us/sample - loss: 1.8483 - accuracy: 0.3245 - val_loss: 1.8795 - val_accuracy: 0.3090\n",
      "Epoch 6/25\n",
      "45000/45000 [==============================] - 11s 252us/sample - loss: 1.8352 - accuracy: 0.3328 - val_loss: 2.0996 - val_accuracy: 0.2736\n",
      "Epoch 7/25\n",
      "45000/45000 [==============================] - 11s 241us/sample - loss: 1.8045 - accuracy: 0.3416 - val_loss: 1.9538 - val_accuracy: 0.3134\n",
      "Epoch 8/25\n",
      "45000/45000 [==============================] - 11s 237us/sample - loss: 1.7769 - accuracy: 0.3544 - val_loss: 1.8124 - val_accuracy: 0.3436\n",
      "Epoch 9/25\n",
      "45000/45000 [==============================] - 11s 235us/sample - loss: 1.7620 - accuracy: 0.3602 - val_loss: 1.7719 - val_accuracy: 0.3554\n",
      "Epoch 10/25\n",
      "45000/45000 [==============================] - 10s 231us/sample - loss: 1.7513 - accuracy: 0.3674 - val_loss: 1.8423 - val_accuracy: 0.3374\n",
      "Epoch 11/25\n",
      "45000/45000 [==============================] - 11s 241us/sample - loss: 1.8012 - accuracy: 0.3570 - val_loss: 1.8385 - val_accuracy: 0.3264\n",
      "Epoch 12/25\n",
      "45000/45000 [==============================] - 11s 242us/sample - loss: 1.9425 - accuracy: 0.2983 - val_loss: 1.9885 - val_accuracy: 0.2242\n",
      "Epoch 13/25\n",
      "45000/45000 [==============================] - 11s 243us/sample - loss: 1.8877 - accuracy: 0.2845 - val_loss: 1.8975 - val_accuracy: 0.2920\n",
      "Epoch 14/25\n",
      "45000/45000 [==============================] - 11s 239us/sample - loss: 1.8378 - accuracy: 0.3170 - val_loss: 1.8653 - val_accuracy: 0.3044\n",
      "Epoch 15/25\n",
      "45000/45000 [==============================] - 11s 237us/sample - loss: 1.8066 - accuracy: 0.3290 - val_loss: 1.8104 - val_accuracy: 0.3312\n",
      "Epoch 16/25\n",
      "45000/45000 [==============================] - 11s 242us/sample - loss: 1.7989 - accuracy: 0.3290 - val_loss: 1.8056 - val_accuracy: 0.3338\n",
      "Epoch 17/25\n",
      "45000/45000 [==============================] - 11s 239us/sample - loss: 1.7808 - accuracy: 0.3410 - val_loss: 1.8187 - val_accuracy: 0.3266\n",
      "Epoch 18/25\n",
      "45000/45000 [==============================] - 11s 253us/sample - loss: 1.7572 - accuracy: 0.3504 - val_loss: 1.7683 - val_accuracy: 0.3506\n",
      "Epoch 19/25\n",
      "45000/45000 [==============================] - 11s 238us/sample - loss: 1.7453 - accuracy: 0.3530 - val_loss: 1.7698 - val_accuracy: 0.3576\n",
      "Epoch 20/25\n",
      "45000/45000 [==============================] - 11s 247us/sample - loss: 1.7359 - accuracy: 0.3622 - val_loss: 1.7649 - val_accuracy: 0.3552\n",
      "Epoch 21/25\n",
      "45000/45000 [==============================] - 11s 236us/sample - loss: 1.7202 - accuracy: 0.3676 - val_loss: 1.7569 - val_accuracy: 0.3594\n",
      "Epoch 22/25\n",
      "45000/45000 [==============================] - 11s 240us/sample - loss: 1.7070 - accuracy: 0.3764 - val_loss: 1.7401 - val_accuracy: 0.3776\n",
      "Epoch 23/25\n",
      "45000/45000 [==============================] - 11s 237us/sample - loss: 1.9523 - accuracy: 0.2878 - val_loss: 1.9603 - val_accuracy: 0.2868\n",
      "Epoch 24/25\n",
      "45000/45000 [==============================] - 11s 244us/sample - loss: 1.8518 - accuracy: 0.3162 - val_loss: 1.8550 - val_accuracy: 0.3326\n",
      "Epoch 25/25\n",
      "45000/45000 [==============================] - 11s 243us/sample - loss: 1.8167 - accuracy: 0.3332 - val_loss: 1.8411 - val_accuracy: 0.3218\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/25\n",
      "45000/45000 [==============================] - 13s 284us/sample - loss: 2.8462 - accuracy: 0.2068 - val_loss: 2.0335 - val_accuracy: 0.2506\n",
      "Epoch 2/25\n",
      "45000/45000 [==============================] - 11s 244us/sample - loss: 1.9454 - accuracy: 0.2856 - val_loss: 1.9431 - val_accuracy: 0.2756\n",
      "Epoch 3/25\n",
      "45000/45000 [==============================] - 11s 245us/sample - loss: 1.8682 - accuracy: 0.3183 - val_loss: 1.9801 - val_accuracy: 0.2980\n",
      "Epoch 4/25\n",
      "45000/45000 [==============================] - 11s 255us/sample - loss: 1.8140 - accuracy: 0.3398 - val_loss: 1.8668 - val_accuracy: 0.3226\n",
      "Epoch 5/25\n",
      "45000/45000 [==============================] - 12s 257us/sample - loss: 1.7651 - accuracy: 0.3578 - val_loss: 2.0365 - val_accuracy: 0.2968\n",
      "Epoch 6/25\n",
      "45000/45000 [==============================] - 11s 253us/sample - loss: 1.7249 - accuracy: 0.3741 - val_loss: 1.7246 - val_accuracy: 0.3888\n",
      "Epoch 7/25\n",
      "45000/45000 [==============================] - 11s 251us/sample - loss: 1.6852 - accuracy: 0.3881 - val_loss: 1.6859 - val_accuracy: 0.3954\n",
      "Epoch 8/25\n",
      "45000/45000 [==============================] - 11s 252us/sample - loss: 1.6487 - accuracy: 0.4022 - val_loss: 1.7547 - val_accuracy: 0.3652\n",
      "Epoch 9/25\n",
      "45000/45000 [==============================] - 11s 243us/sample - loss: 1.6294 - accuracy: 0.4112 - val_loss: 1.6964 - val_accuracy: 0.3912\n",
      "Epoch 10/25\n",
      "45000/45000 [==============================] - 11s 253us/sample - loss: 1.6028 - accuracy: 0.4202 - val_loss: 1.6503 - val_accuracy: 0.4154\n",
      "Epoch 11/25\n",
      "45000/45000 [==============================] - 11s 245us/sample - loss: 1.5804 - accuracy: 0.4294 - val_loss: 1.6668 - val_accuracy: 0.4018\n",
      "Epoch 12/25\n",
      "45000/45000 [==============================] - 11s 246us/sample - loss: 1.5641 - accuracy: 0.4375 - val_loss: 1.6701 - val_accuracy: 0.3944\n",
      "Epoch 13/25\n",
      "45000/45000 [==============================] - 11s 251us/sample - loss: 1.5478 - accuracy: 0.4455 - val_loss: 1.6252 - val_accuracy: 0.4290\n",
      "Epoch 14/25\n",
      "45000/45000 [==============================] - 11s 252us/sample - loss: 1.5285 - accuracy: 0.4501 - val_loss: 1.5905 - val_accuracy: 0.4378\n",
      "Epoch 15/25\n",
      "45000/45000 [==============================] - 11s 249us/sample - loss: 1.5164 - accuracy: 0.4564 - val_loss: 1.5603 - val_accuracy: 0.4488\n",
      "Epoch 16/25\n",
      "45000/45000 [==============================] - 11s 243us/sample - loss: 1.4997 - accuracy: 0.4628 - val_loss: 1.7257 - val_accuracy: 0.3910\n",
      "Epoch 17/25\n",
      "45000/45000 [==============================] - 11s 246us/sample - loss: 1.4932 - accuracy: 0.4664 - val_loss: 1.5600 - val_accuracy: 0.4452\n",
      "Epoch 18/25\n",
      "45000/45000 [==============================] - 11s 241us/sample - loss: 1.4749 - accuracy: 0.4701 - val_loss: 1.5776 - val_accuracy: 0.4468\n",
      "Epoch 19/25\n",
      "45000/45000 [==============================] - 11s 245us/sample - loss: 1.4618 - accuracy: 0.4752 - val_loss: 1.6177 - val_accuracy: 0.4404\n",
      "Epoch 20/25\n",
      "45000/45000 [==============================] - 12s 256us/sample - loss: 1.4534 - accuracy: 0.4772 - val_loss: 1.6031 - val_accuracy: 0.4290\n",
      "Epoch 21/25\n",
      "45000/45000 [==============================] - 11s 246us/sample - loss: 1.4366 - accuracy: 0.4832 - val_loss: 1.5613 - val_accuracy: 0.4634\n",
      "Epoch 22/25\n",
      "45000/45000 [==============================] - 11s 243us/sample - loss: 1.4316 - accuracy: 0.4845 - val_loss: 1.5677 - val_accuracy: 0.4620\n",
      "Epoch 23/25\n",
      "45000/45000 [==============================] - 11s 234us/sample - loss: 1.4246 - accuracy: 0.4894 - val_loss: 1.6136 - val_accuracy: 0.4330\n",
      "Epoch 24/25\n",
      "45000/45000 [==============================] - 11s 242us/sample - loss: 1.4080 - accuracy: 0.4936 - val_loss: 1.5591 - val_accuracy: 0.4568\n",
      "Epoch 25/25\n",
      "45000/45000 [==============================] - 10s 231us/sample - loss: 1.3982 - accuracy: 0.4982 - val_loss: 1.5993 - val_accuracy: 0.4516\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/25\n",
      "45000/45000 [==============================] - 12s 270us/sample - loss: 3.2861 - accuracy: 0.1786 - val_loss: 2.4411 - val_accuracy: 0.1728\n",
      "Epoch 2/25\n",
      "45000/45000 [==============================] - 11s 241us/sample - loss: 1.9729 - accuracy: 0.2762 - val_loss: 2.0566 - val_accuracy: 0.2518\n",
      "Epoch 3/25\n",
      "45000/45000 [==============================] - 11s 241us/sample - loss: 1.8699 - accuracy: 0.3192 - val_loss: 2.0191 - val_accuracy: 0.2668\n",
      "Epoch 4/25\n",
      "45000/45000 [==============================] - 11s 234us/sample - loss: 1.8079 - accuracy: 0.3451 - val_loss: 1.7920 - val_accuracy: 0.3502\n",
      "Epoch 5/25\n",
      "45000/45000 [==============================] - 11s 235us/sample - loss: 1.7510 - accuracy: 0.3636 - val_loss: 1.8984 - val_accuracy: 0.3390\n",
      "Epoch 6/25\n",
      "45000/45000 [==============================] - 11s 242us/sample - loss: 1.7040 - accuracy: 0.3844 - val_loss: 1.7034 - val_accuracy: 0.3942\n",
      "Epoch 7/25\n",
      "45000/45000 [==============================] - 11s 239us/sample - loss: 1.6585 - accuracy: 0.4004 - val_loss: 1.7549 - val_accuracy: 0.3644\n",
      "Epoch 8/25\n",
      "45000/45000 [==============================] - 11s 254us/sample - loss: 1.6285 - accuracy: 0.4098 - val_loss: 1.7399 - val_accuracy: 0.3864\n",
      "Epoch 9/25\n",
      "45000/45000 [==============================] - 11s 239us/sample - loss: 1.5960 - accuracy: 0.4255 - val_loss: 1.6525 - val_accuracy: 0.4002\n",
      "Epoch 10/25\n",
      "45000/45000 [==============================] - 11s 246us/sample - loss: 1.5728 - accuracy: 0.4314 - val_loss: 1.6215 - val_accuracy: 0.4270\n",
      "Epoch 11/25\n",
      "45000/45000 [==============================] - 11s 247us/sample - loss: 1.5518 - accuracy: 0.4426 - val_loss: 1.6196 - val_accuracy: 0.4296\n",
      "Epoch 12/25\n",
      "45000/45000 [==============================] - 11s 246us/sample - loss: 1.5336 - accuracy: 0.4477 - val_loss: 1.6116 - val_accuracy: 0.4292\n",
      "Epoch 13/25\n",
      "45000/45000 [==============================] - 11s 252us/sample - loss: 1.5185 - accuracy: 0.4537 - val_loss: 1.6059 - val_accuracy: 0.4392\n",
      "Epoch 14/25\n",
      "45000/45000 [==============================] - 11s 254us/sample - loss: 1.5007 - accuracy: 0.4615 - val_loss: 1.6023 - val_accuracy: 0.4380\n",
      "Epoch 15/25\n",
      "45000/45000 [==============================] - 11s 240us/sample - loss: 1.4848 - accuracy: 0.4670 - val_loss: 1.6337 - val_accuracy: 0.4146\n",
      "Epoch 16/25\n",
      "45000/45000 [==============================] - 11s 244us/sample - loss: 1.4733 - accuracy: 0.4710 - val_loss: 1.6841 - val_accuracy: 0.4168\n",
      "Epoch 17/25\n",
      "45000/45000 [==============================] - 11s 248us/sample - loss: 1.4580 - accuracy: 0.4783 - val_loss: 1.5963 - val_accuracy: 0.4436\n",
      "Epoch 18/25\n",
      "45000/45000 [==============================] - 11s 247us/sample - loss: 1.4444 - accuracy: 0.4823 - val_loss: 1.5925 - val_accuracy: 0.4414\n",
      "Epoch 19/25\n",
      "45000/45000 [==============================] - 11s 249us/sample - loss: 1.4322 - accuracy: 0.4851 - val_loss: 1.5593 - val_accuracy: 0.4538\n",
      "Epoch 20/25\n",
      "45000/45000 [==============================] - 11s 246us/sample - loss: 1.4232 - accuracy: 0.4915 - val_loss: 1.6096 - val_accuracy: 0.4374\n",
      "Epoch 21/25\n",
      "45000/45000 [==============================] - 11s 242us/sample - loss: 1.4075 - accuracy: 0.4946 - val_loss: 1.5676 - val_accuracy: 0.4516\n",
      "Epoch 22/25\n",
      "45000/45000 [==============================] - 11s 253us/sample - loss: 1.3973 - accuracy: 0.4984 - val_loss: 1.5706 - val_accuracy: 0.4460\n",
      "Epoch 23/25\n",
      "45000/45000 [==============================] - 12s 260us/sample - loss: 1.3836 - accuracy: 0.5054 - val_loss: 1.5513 - val_accuracy: 0.4638\n",
      "Epoch 24/25\n",
      "45000/45000 [==============================] - 12s 258us/sample - loss: 1.3730 - accuracy: 0.5078 - val_loss: 1.5684 - val_accuracy: 0.4494\n",
      "Epoch 25/25\n",
      "45000/45000 [==============================] - 11s 244us/sample - loss: 1.3663 - accuracy: 0.5102 - val_loss: 1.5866 - val_accuracy: 0.4444\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/25\n",
      "45000/45000 [==============================] - 13s 285us/sample - loss: 4.9835 - accuracy: 0.1465 - val_loss: 2.2695 - val_accuracy: 0.2042\n",
      "Epoch 2/25\n",
      "45000/45000 [==============================] - 11s 244us/sample - loss: 2.1398 - accuracy: 0.2279 - val_loss: 2.0677 - val_accuracy: 0.2562\n",
      "Epoch 3/25\n",
      "45000/45000 [==============================] - 11s 242us/sample - loss: 1.9991 - accuracy: 0.2677 - val_loss: 1.9831 - val_accuracy: 0.2660\n",
      "Epoch 4/25\n",
      "45000/45000 [==============================] - 11s 248us/sample - loss: 1.9147 - accuracy: 0.2991 - val_loss: 1.8925 - val_accuracy: 0.3158\n",
      "Epoch 5/25\n",
      "45000/45000 [==============================] - 11s 247us/sample - loss: 1.8520 - accuracy: 0.3209 - val_loss: 1.8553 - val_accuracy: 0.3268\n",
      "Epoch 6/25\n",
      "45000/45000 [==============================] - 11s 252us/sample - loss: 1.8017 - accuracy: 0.3399 - val_loss: 1.8372 - val_accuracy: 0.3344\n",
      "Epoch 7/25\n",
      "45000/45000 [==============================] - 12s 260us/sample - loss: 1.7610 - accuracy: 0.3568 - val_loss: 1.8421 - val_accuracy: 0.3326\n",
      "Epoch 8/25\n",
      "45000/45000 [==============================] - 12s 261us/sample - loss: 1.7231 - accuracy: 0.3715 - val_loss: 1.7344 - val_accuracy: 0.3816\n",
      "Epoch 9/25\n",
      "45000/45000 [==============================] - 11s 252us/sample - loss: 1.6890 - accuracy: 0.3858 - val_loss: 1.8247 - val_accuracy: 0.3532\n",
      "Epoch 10/25\n",
      "45000/45000 [==============================] - 20s 446us/sample - loss: 1.6646 - accuracy: 0.3951 - val_loss: 1.7000 - val_accuracy: 0.3946\n",
      "Epoch 11/25\n",
      "45000/45000 [==============================] - 19s 417us/sample - loss: 1.6370 - accuracy: 0.4044 - val_loss: 1.6935 - val_accuracy: 0.3934\n",
      "Epoch 12/25\n",
      "45000/45000 [==============================] - 13s 290us/sample - loss: 1.6138 - accuracy: 0.4155 - val_loss: 1.6812 - val_accuracy: 0.4012\n",
      "Epoch 13/25\n",
      "45000/45000 [==============================] - 11s 248us/sample - loss: 1.5912 - accuracy: 0.4254 - val_loss: 1.6670 - val_accuracy: 0.3990\n",
      "Epoch 14/25\n",
      "45000/45000 [==============================] - 20s 438us/sample - loss: 1.5788 - accuracy: 0.4282 - val_loss: 1.6822 - val_accuracy: 0.4018\n",
      "Epoch 15/25\n",
      "45000/45000 [==============================] - 12s 262us/sample - loss: 1.5584 - accuracy: 0.4361 - val_loss: 1.6679 - val_accuracy: 0.4146\n",
      "Epoch 16/25\n",
      "45000/45000 [==============================] - 20s 445us/sample - loss: 1.5435 - accuracy: 0.4415 - val_loss: 1.6519 - val_accuracy: 0.4124\n",
      "Epoch 17/25\n",
      "45000/45000 [==============================] - 21s 456us/sample - loss: 1.5264 - accuracy: 0.4471 - val_loss: 1.6407 - val_accuracy: 0.4230\n",
      "Epoch 18/25\n",
      "45000/45000 [==============================] - 20s 451us/sample - loss: 1.5144 - accuracy: 0.4537 - val_loss: 1.6386 - val_accuracy: 0.4244\n",
      "Epoch 19/25\n",
      "45000/45000 [==============================] - 12s 258us/sample - loss: 1.5026 - accuracy: 0.4570 - val_loss: 1.6653 - val_accuracy: 0.4204\n",
      "Epoch 20/25\n",
      "45000/45000 [==============================] - 11s 253us/sample - loss: 1.4905 - accuracy: 0.4630 - val_loss: 1.6073 - val_accuracy: 0.4300\n",
      "Epoch 21/25\n",
      "45000/45000 [==============================] - 11s 237us/sample - loss: 1.4787 - accuracy: 0.4662 - val_loss: 1.6255 - val_accuracy: 0.4244\n",
      "Epoch 22/25\n",
      "45000/45000 [==============================] - 19s 430us/sample - loss: 1.4690 - accuracy: 0.4708 - val_loss: 1.6337 - val_accuracy: 0.4240\n",
      "Epoch 23/25\n",
      "45000/45000 [==============================] - 11s 254us/sample - loss: 1.4530 - accuracy: 0.4741 - val_loss: 1.5875 - val_accuracy: 0.4420\n",
      "Epoch 24/25\n",
      "45000/45000 [==============================] - 12s 257us/sample - loss: 1.4463 - accuracy: 0.4793 - val_loss: 1.6109 - val_accuracy: 0.4354\n",
      "Epoch 25/25\n",
      "45000/45000 [==============================] - 11s 250us/sample - loss: 1.4357 - accuracy: 0.4828 - val_loss: 1.6171 - val_accuracy: 0.4306\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/25\n",
      "45000/45000 [==============================] - 22s 489us/sample - loss: 9.6926 - accuracy: 0.1167 - val_loss: 3.3922 - val_accuracy: 0.1406\n",
      "Epoch 2/25\n",
      "45000/45000 [==============================] - 11s 250us/sample - loss: 2.8892 - accuracy: 0.1481 - val_loss: 2.5534 - val_accuracy: 0.1546\n",
      "Epoch 3/25\n",
      "45000/45000 [==============================] - 12s 268us/sample - loss: 2.3978 - accuracy: 0.1800 - val_loss: 2.2818 - val_accuracy: 0.2038\n",
      "Epoch 4/25\n",
      "45000/45000 [==============================] - 12s 260us/sample - loss: 2.1981 - accuracy: 0.2150 - val_loss: 2.1391 - val_accuracy: 0.2300\n",
      "Epoch 5/25\n",
      "45000/45000 [==============================] - 12s 262us/sample - loss: 2.0864 - accuracy: 0.2428 - val_loss: 2.0681 - val_accuracy: 0.2520\n",
      "Epoch 6/25\n",
      "45000/45000 [==============================] - 12s 263us/sample - loss: 2.0030 - accuracy: 0.2735 - val_loss: 1.9715 - val_accuracy: 0.2852\n",
      "Epoch 7/25\n",
      "45000/45000 [==============================] - 12s 256us/sample - loss: 1.9421 - accuracy: 0.2899 - val_loss: 1.9710 - val_accuracy: 0.2980\n",
      "Epoch 8/25\n",
      "45000/45000 [==============================] - 12s 256us/sample - loss: 1.8898 - accuracy: 0.3120 - val_loss: 1.9042 - val_accuracy: 0.3192\n",
      "Epoch 9/25\n",
      "45000/45000 [==============================] - 17s 373us/sample - loss: 1.8493 - accuracy: 0.3275 - val_loss: 1.8661 - val_accuracy: 0.3340\n",
      "Epoch 10/25\n",
      "45000/45000 [==============================] - 14s 308us/sample - loss: 1.8178 - accuracy: 0.3400 - val_loss: 1.8402 - val_accuracy: 0.3430\n",
      "Epoch 11/25\n",
      "45000/45000 [==============================] - 11s 251us/sample - loss: 1.7933 - accuracy: 0.3488 - val_loss: 1.8287 - val_accuracy: 0.3536\n",
      "Epoch 12/25\n",
      "45000/45000 [==============================] - 20s 439us/sample - loss: 1.7714 - accuracy: 0.3587 - val_loss: 1.8248 - val_accuracy: 0.3598\n",
      "Epoch 13/25\n",
      "45000/45000 [==============================] - 11s 253us/sample - loss: 1.7501 - accuracy: 0.3671 - val_loss: 1.8105 - val_accuracy: 0.3526\n",
      "Epoch 14/25\n",
      "45000/45000 [==============================] - 19s 429us/sample - loss: 1.7308 - accuracy: 0.3739 - val_loss: 1.7730 - val_accuracy: 0.3700\n",
      "Epoch 15/25\n",
      "45000/45000 [==============================] - 11s 255us/sample - loss: 1.7124 - accuracy: 0.3812 - val_loss: 1.7862 - val_accuracy: 0.3746\n",
      "Epoch 16/25\n",
      "45000/45000 [==============================] - 12s 262us/sample - loss: 1.6967 - accuracy: 0.3869 - val_loss: 1.7735 - val_accuracy: 0.3616\n",
      "Epoch 17/25\n",
      "45000/45000 [==============================] - 11s 248us/sample - loss: 1.6840 - accuracy: 0.3922 - val_loss: 1.7514 - val_accuracy: 0.3788\n",
      "Epoch 18/25\n",
      "45000/45000 [==============================] - 19s 427us/sample - loss: 1.6686 - accuracy: 0.3988 - val_loss: 1.7474 - val_accuracy: 0.3864\n",
      "Epoch 19/25\n",
      "45000/45000 [==============================] - 20s 438us/sample - loss: 1.6538 - accuracy: 0.4015 - val_loss: 1.7508 - val_accuracy: 0.3790\n",
      "Epoch 20/25\n",
      "45000/45000 [==============================] - 11s 252us/sample - loss: 1.6399 - accuracy: 0.4079 - val_loss: 1.7265 - val_accuracy: 0.3858\n",
      "Epoch 21/25\n",
      "45000/45000 [==============================] - 11s 251us/sample - loss: 1.6287 - accuracy: 0.4143 - val_loss: 1.7204 - val_accuracy: 0.3926\n",
      "Epoch 22/25\n",
      "45000/45000 [==============================] - 11s 252us/sample - loss: 1.6177 - accuracy: 0.4159 - val_loss: 1.7349 - val_accuracy: 0.3828\n",
      "Epoch 23/25\n",
      "45000/45000 [==============================] - 20s 435us/sample - loss: 1.6062 - accuracy: 0.4203 - val_loss: 1.7152 - val_accuracy: 0.3920\n",
      "Epoch 24/25\n",
      "45000/45000 [==============================] - 11s 255us/sample - loss: 1.5943 - accuracy: 0.4256 - val_loss: 1.6969 - val_accuracy: 0.3948\n",
      "Epoch 25/25\n",
      "45000/45000 [==============================] - 16s 350us/sample - loss: 1.5841 - accuracy: 0.4294 - val_loss: 1.6977 - val_accuracy: 0.4020\n"
     ]
    }
   ],
   "source": [
    "index_run = 0\n",
    "Wsave = model.get_weights()\n",
    "for lr in range(4,11):\n",
    "    lr = 10**(-lr/2)\n",
    "    index_run += 1\n",
    "    run_logdir = os.path.join(os.curdir, \"my_cifar10_logs\", \"run_{:03d}\".format(index_run))\n",
    "    tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "    callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n",
    "    optimizer = keras.optimizers.Nadam(lr=lr)\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "    model.fit(X_train, y_train, epochs=25, validation_data=(X_valid, y_valid), callbacks=callbacks)\n",
    "    model.set_weights(Wsave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "best_lr = 10**(-8/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=X_train.shape[1:]))\n",
    "for i in range(20):\n",
    "    model.add(keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_cifar10_model.h5\", save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = best_lr\n",
    "index_run += 1\n",
    "run_logdir = os.path.join(os.curdir, \"my_cifar10_logs\", \"run_{:03d}\".format(index_run))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n",
    "optimizer = keras.optimizers.Nadam(lr=lr)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/100\n",
      "45000/45000 [==============================] - 12s 265us/sample - loss: 3.3934 - accuracy: 0.1833 - val_loss: 2.1382 - val_accuracy: 0.2414\n",
      "Epoch 2/100\n",
      "45000/45000 [==============================] - 10s 219us/sample - loss: 1.9744 - accuracy: 0.2704 - val_loss: 1.9653 - val_accuracy: 0.2812\n",
      "Epoch 3/100\n",
      "45000/45000 [==============================] - 10s 220us/sample - loss: 1.8706 - accuracy: 0.3119 - val_loss: 1.8616 - val_accuracy: 0.3120\n",
      "Epoch 4/100\n",
      "45000/45000 [==============================] - 10s 230us/sample - loss: 1.8000 - accuracy: 0.3421 - val_loss: 1.8221 - val_accuracy: 0.3420\n",
      "Epoch 5/100\n",
      "45000/45000 [==============================] - 10s 215us/sample - loss: 1.7418 - accuracy: 0.3676 - val_loss: 1.7353 - val_accuracy: 0.3834\n",
      "Epoch 6/100\n",
      "45000/45000 [==============================] - 10s 213us/sample - loss: 1.6920 - accuracy: 0.3847 - val_loss: 1.6922 - val_accuracy: 0.3894\n",
      "Epoch 7/100\n",
      "45000/45000 [==============================] - 10s 223us/sample - loss: 1.6489 - accuracy: 0.4030 - val_loss: 1.7135 - val_accuracy: 0.3864\n",
      "Epoch 8/100\n",
      "45000/45000 [==============================] - 10s 217us/sample - loss: 1.6085 - accuracy: 0.4196 - val_loss: 1.6518 - val_accuracy: 0.4122\n",
      "Epoch 9/100\n",
      "45000/45000 [==============================] - 9s 209us/sample - loss: 1.5832 - accuracy: 0.4276 - val_loss: 1.6086 - val_accuracy: 0.4274\n",
      "Epoch 10/100\n",
      "45000/45000 [==============================] - 10s 217us/sample - loss: 1.5584 - accuracy: 0.4361 - val_loss: 1.6253 - val_accuracy: 0.4134\n",
      "Epoch 11/100\n",
      "45000/45000 [==============================] - 10s 214us/sample - loss: 1.5380 - accuracy: 0.4447 - val_loss: 1.6551 - val_accuracy: 0.4130\n",
      "Epoch 12/100\n",
      "45000/45000 [==============================] - 10s 216us/sample - loss: 1.5176 - accuracy: 0.4516 - val_loss: 1.5942 - val_accuracy: 0.4388\n",
      "Epoch 13/100\n",
      "45000/45000 [==============================] - 10s 220us/sample - loss: 1.4975 - accuracy: 0.4600 - val_loss: 1.6109 - val_accuracy: 0.4316\n",
      "Epoch 14/100\n",
      "45000/45000 [==============================] - 10s 216us/sample - loss: 1.4789 - accuracy: 0.4678 - val_loss: 1.6126 - val_accuracy: 0.4380\n",
      "Epoch 15/100\n",
      "45000/45000 [==============================] - 9s 210us/sample - loss: 1.4618 - accuracy: 0.4727 - val_loss: 1.5759 - val_accuracy: 0.4500\n",
      "Epoch 16/100\n",
      "45000/45000 [==============================] - 10s 216us/sample - loss: 1.4465 - accuracy: 0.4773 - val_loss: 1.5505 - val_accuracy: 0.4602\n",
      "Epoch 17/100\n",
      "45000/45000 [==============================] - 10s 223us/sample - loss: 1.4333 - accuracy: 0.4838 - val_loss: 1.5801 - val_accuracy: 0.4508\n",
      "Epoch 18/100\n",
      "45000/45000 [==============================] - 9s 206us/sample - loss: 1.4179 - accuracy: 0.4903 - val_loss: 1.5491 - val_accuracy: 0.4622\n",
      "Epoch 19/100\n",
      "45000/45000 [==============================] - 10s 215us/sample - loss: 1.4029 - accuracy: 0.4954 - val_loss: 1.5654 - val_accuracy: 0.4594\n",
      "Epoch 20/100\n",
      "45000/45000 [==============================] - 10s 211us/sample - loss: 1.3950 - accuracy: 0.4957 - val_loss: 1.5306 - val_accuracy: 0.4644\n",
      "Epoch 21/100\n",
      "45000/45000 [==============================] - 9s 209us/sample - loss: 1.3787 - accuracy: 0.5018 - val_loss: 1.5775 - val_accuracy: 0.4470\n",
      "Epoch 22/100\n",
      "45000/45000 [==============================] - 10s 216us/sample - loss: 1.3699 - accuracy: 0.5050 - val_loss: 1.5107 - val_accuracy: 0.4658\n",
      "Epoch 23/100\n",
      "45000/45000 [==============================] - 10s 215us/sample - loss: 1.3559 - accuracy: 0.5072 - val_loss: 1.5044 - val_accuracy: 0.4668\n",
      "Epoch 24/100\n",
      "45000/45000 [==============================] - 9s 208us/sample - loss: 1.3486 - accuracy: 0.5147 - val_loss: 1.5698 - val_accuracy: 0.4478\n",
      "Epoch 25/100\n",
      "45000/45000 [==============================] - 9s 207us/sample - loss: 1.3337 - accuracy: 0.5192 - val_loss: 1.5549 - val_accuracy: 0.4568\n",
      "Epoch 26/100\n",
      "45000/45000 [==============================] - 10s 215us/sample - loss: 1.3275 - accuracy: 0.5192 - val_loss: 1.5623 - val_accuracy: 0.4670\n",
      "Epoch 27/100\n",
      "45000/45000 [==============================] - 9s 207us/sample - loss: 1.3151 - accuracy: 0.5265 - val_loss: 1.5144 - val_accuracy: 0.4702\n",
      "Epoch 28/100\n",
      "45000/45000 [==============================] - 9s 203us/sample - loss: 1.3024 - accuracy: 0.5284 - val_loss: 1.5375 - val_accuracy: 0.4664\n",
      "Epoch 29/100\n",
      "45000/45000 [==============================] - 9s 208us/sample - loss: 1.2906 - accuracy: 0.5328 - val_loss: 1.5276 - val_accuracy: 0.4706\n",
      "Epoch 30/100\n",
      "45000/45000 [==============================] - 9s 202us/sample - loss: 1.2834 - accuracy: 0.5354 - val_loss: 1.4952 - val_accuracy: 0.4800\n",
      "Epoch 31/100\n",
      "45000/45000 [==============================] - 9s 204us/sample - loss: 1.2743 - accuracy: 0.5398 - val_loss: 1.5414 - val_accuracy: 0.4730\n",
      "Epoch 32/100\n",
      "45000/45000 [==============================] - 9s 202us/sample - loss: 1.2632 - accuracy: 0.5420 - val_loss: 1.5484 - val_accuracy: 0.4762\n",
      "Epoch 33/100\n",
      "45000/45000 [==============================] - 9s 208us/sample - loss: 1.2557 - accuracy: 0.5459 - val_loss: 1.5070 - val_accuracy: 0.4720\n",
      "Epoch 34/100\n",
      "45000/45000 [==============================] - 9s 200us/sample - loss: 1.2438 - accuracy: 0.5496 - val_loss: 1.5168 - val_accuracy: 0.4702\n",
      "Epoch 35/100\n",
      "45000/45000 [==============================] - 9s 209us/sample - loss: 1.2375 - accuracy: 0.5494 - val_loss: 1.4946 - val_accuracy: 0.4866\n",
      "Epoch 36/100\n",
      "45000/45000 [==============================] - 9s 202us/sample - loss: 1.2232 - accuracy: 0.5564 - val_loss: 1.5349 - val_accuracy: 0.4768\n",
      "Epoch 37/100\n",
      "45000/45000 [==============================] - 9s 205us/sample - loss: 1.2159 - accuracy: 0.5592 - val_loss: 1.5791 - val_accuracy: 0.4590\n",
      "Epoch 38/100\n",
      "45000/45000 [==============================] - 9s 202us/sample - loss: 1.2087 - accuracy: 0.5594 - val_loss: 1.5100 - val_accuracy: 0.4888\n",
      "Epoch 39/100\n",
      "45000/45000 [==============================] - 9s 205us/sample - loss: 1.2015 - accuracy: 0.5665 - val_loss: 1.5368 - val_accuracy: 0.4746\n",
      "Epoch 40/100\n",
      "45000/45000 [==============================] - 9s 199us/sample - loss: 1.1907 - accuracy: 0.5686 - val_loss: 1.5825 - val_accuracy: 0.4738\n",
      "Epoch 41/100\n",
      "45000/45000 [==============================] - 9s 204us/sample - loss: 1.1805 - accuracy: 0.5718 - val_loss: 1.5525 - val_accuracy: 0.4686\n",
      "Epoch 42/100\n",
      "45000/45000 [==============================] - 9s 193us/sample - loss: 1.1766 - accuracy: 0.5761 - val_loss: 1.5235 - val_accuracy: 0.4712\n",
      "Epoch 43/100\n",
      "45000/45000 [==============================] - 9s 196us/sample - loss: 1.1643 - accuracy: 0.5761 - val_loss: 1.5353 - val_accuracy: 0.4908\n",
      "Epoch 44/100\n",
      "45000/45000 [==============================] - 9s 194us/sample - loss: 1.1558 - accuracy: 0.5831 - val_loss: 1.5690 - val_accuracy: 0.4660\n",
      "Epoch 45/100\n",
      "45000/45000 [==============================] - 9s 195us/sample - loss: 1.1527 - accuracy: 0.5817 - val_loss: 1.5617 - val_accuracy: 0.4830\n",
      "Epoch 46/100\n",
      "45000/45000 [==============================] - 9s 198us/sample - loss: 1.1451 - accuracy: 0.5855 - val_loss: 1.5393 - val_accuracy: 0.4862\n",
      "Epoch 47/100\n",
      "45000/45000 [==============================] - 9s 196us/sample - loss: 1.1371 - accuracy: 0.5877 - val_loss: 1.5657 - val_accuracy: 0.4726\n",
      "Epoch 48/100\n",
      "45000/45000 [==============================] - 9s 202us/sample - loss: 1.1257 - accuracy: 0.5901 - val_loss: 1.5523 - val_accuracy: 0.4860\n",
      "Epoch 49/100\n",
      "45000/45000 [==============================] - 9s 201us/sample - loss: 1.1190 - accuracy: 0.5950 - val_loss: 1.5410 - val_accuracy: 0.4830\n",
      "Epoch 50/100\n",
      "45000/45000 [==============================] - 9s 202us/sample - loss: 1.1108 - accuracy: 0.5967 - val_loss: 1.5761 - val_accuracy: 0.4842\n",
      "Epoch 51/100\n",
      "45000/45000 [==============================] - 9s 194us/sample - loss: 1.1049 - accuracy: 0.6008 - val_loss: 1.5612 - val_accuracy: 0.4852\n",
      "Epoch 52/100\n",
      "45000/45000 [==============================] - 9s 197us/sample - loss: 1.0968 - accuracy: 0.6019 - val_loss: 1.5605 - val_accuracy: 0.4766\n",
      "Epoch 53/100\n",
      "45000/45000 [==============================] - 9s 195us/sample - loss: 1.0894 - accuracy: 0.6034 - val_loss: 1.5461 - val_accuracy: 0.4806\n",
      "Epoch 54/100\n",
      "45000/45000 [==============================] - 9s 197us/sample - loss: 1.0771 - accuracy: 0.6107 - val_loss: 1.6021 - val_accuracy: 0.4832\n",
      "Epoch 55/100\n",
      "45000/45000 [==============================] - 9s 194us/sample - loss: 1.0741 - accuracy: 0.6097 - val_loss: 1.6040 - val_accuracy: 0.4628\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f4fd4712190>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 0s 71us/sample - loss: 1.4946 - accuracy: 0.4866\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.4945957498550415, 0.4866]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.load_model(\"my_cifar10_model.h5\")\n",
    "model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Problem 8c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Now try adding Batch Normalization and compare the learning curves: Is it\n",
    "converging faster than before? Does it produce a better model? How does it affect training speed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=X_train.shape[1:]))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100, kernel_initializer=\"he_normal\"))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation(\"elu\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Nadam(lr=best_lr)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_cifar10_bn_model.h5\", save_best_only=True)\n",
    "index_run += 1\n",
    "run_logdir = os.path.join(os.curdir, \"my_cifar10_logs\", \"run_bn_{:03d}\".format(index_run))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/100\n",
      "45000/45000 [==============================] - 25s 554us/sample - loss: 1.9263 - accuracy: 0.3115 - val_loss: 1.6841 - val_accuracy: 0.4020\n",
      "Epoch 2/100\n",
      "45000/45000 [==============================] - 20s 438us/sample - loss: 1.7041 - accuracy: 0.3916 - val_loss: 1.5915 - val_accuracy: 0.4434\n",
      "Epoch 3/100\n",
      "45000/45000 [==============================] - 20s 439us/sample - loss: 1.6298 - accuracy: 0.4197 - val_loss: 1.5677 - val_accuracy: 0.4442\n",
      "Epoch 4/100\n",
      "45000/45000 [==============================] - 20s 442us/sample - loss: 1.5755 - accuracy: 0.4404 - val_loss: 1.5076 - val_accuracy: 0.4728\n",
      "Epoch 5/100\n",
      "45000/45000 [==============================] - 20s 443us/sample - loss: 1.5334 - accuracy: 0.4529 - val_loss: 1.4973 - val_accuracy: 0.4808\n",
      "Epoch 6/100\n",
      "45000/45000 [==============================] - 20s 449us/sample - loss: 1.4974 - accuracy: 0.4673 - val_loss: 1.4630 - val_accuracy: 0.4860\n",
      "Epoch 7/100\n",
      "45000/45000 [==============================] - 20s 448us/sample - loss: 1.4703 - accuracy: 0.4787 - val_loss: 1.4591 - val_accuracy: 0.4854\n",
      "Epoch 8/100\n",
      "45000/45000 [==============================] - 20s 448us/sample - loss: 1.4424 - accuracy: 0.4864 - val_loss: 1.4618 - val_accuracy: 0.4870\n",
      "Epoch 9/100\n",
      "45000/45000 [==============================] - 20s 452us/sample - loss: 1.4196 - accuracy: 0.4986 - val_loss: 1.4206 - val_accuracy: 0.5028\n",
      "Epoch 10/100\n",
      "45000/45000 [==============================] - 21s 457us/sample - loss: 1.3980 - accuracy: 0.5052 - val_loss: 1.4123 - val_accuracy: 0.5056\n",
      "Epoch 11/100\n",
      "45000/45000 [==============================] - 21s 461us/sample - loss: 1.3757 - accuracy: 0.5134 - val_loss: 1.4122 - val_accuracy: 0.5022\n",
      "Epoch 12/100\n",
      "45000/45000 [==============================] - 21s 456us/sample - loss: 1.3571 - accuracy: 0.5192 - val_loss: 1.4036 - val_accuracy: 0.5098\n",
      "Epoch 13/100\n",
      "45000/45000 [==============================] - 20s 452us/sample - loss: 1.3376 - accuracy: 0.5242 - val_loss: 1.4042 - val_accuracy: 0.5168\n",
      "Epoch 14/100\n",
      "45000/45000 [==============================] - 20s 449us/sample - loss: 1.3199 - accuracy: 0.5320 - val_loss: 1.4067 - val_accuracy: 0.5180\n",
      "Epoch 15/100\n",
      "45000/45000 [==============================] - 21s 458us/sample - loss: 1.3105 - accuracy: 0.5366 - val_loss: 1.3931 - val_accuracy: 0.5200\n",
      "Epoch 16/100\n",
      "45000/45000 [==============================] - 21s 459us/sample - loss: 1.2941 - accuracy: 0.5423 - val_loss: 1.3832 - val_accuracy: 0.5180\n",
      "Epoch 17/100\n",
      "45000/45000 [==============================] - 20s 451us/sample - loss: 1.2809 - accuracy: 0.5463 - val_loss: 1.3780 - val_accuracy: 0.5284\n",
      "Epoch 18/100\n",
      "45000/45000 [==============================] - 20s 452us/sample - loss: 1.2657 - accuracy: 0.5476 - val_loss: 1.3753 - val_accuracy: 0.5234\n",
      "Epoch 19/100\n",
      "45000/45000 [==============================] - 20s 454us/sample - loss: 1.2522 - accuracy: 0.5555 - val_loss: 1.3735 - val_accuracy: 0.5252\n",
      "Epoch 20/100\n",
      "45000/45000 [==============================] - 21s 456us/sample - loss: 1.2350 - accuracy: 0.5624 - val_loss: 1.3862 - val_accuracy: 0.5214\n",
      "Epoch 21/100\n",
      "45000/45000 [==============================] - 20s 451us/sample - loss: 1.2235 - accuracy: 0.5680 - val_loss: 1.3855 - val_accuracy: 0.5222\n",
      "Epoch 22/100\n",
      "45000/45000 [==============================] - 20s 454us/sample - loss: 1.2221 - accuracy: 0.5683 - val_loss: 1.3632 - val_accuracy: 0.5290\n",
      "Epoch 23/100\n",
      "45000/45000 [==============================] - 20s 454us/sample - loss: 1.2056 - accuracy: 0.5722 - val_loss: 1.3779 - val_accuracy: 0.5220\n",
      "Epoch 24/100\n",
      "45000/45000 [==============================] - 20s 454us/sample - loss: 1.2001 - accuracy: 0.5729 - val_loss: 1.3705 - val_accuracy: 0.5300\n",
      "Epoch 25/100\n",
      "45000/45000 [==============================] - 20s 454us/sample - loss: 1.1842 - accuracy: 0.5792 - val_loss: 1.3679 - val_accuracy: 0.5334\n",
      "Epoch 26/100\n",
      "45000/45000 [==============================] - 21s 459us/sample - loss: 1.1735 - accuracy: 0.5844 - val_loss: 1.3901 - val_accuracy: 0.5208\n",
      "Epoch 27/100\n",
      "45000/45000 [==============================] - 20s 455us/sample - loss: 1.1595 - accuracy: 0.5900 - val_loss: 1.3705 - val_accuracy: 0.5270\n",
      "Epoch 28/100\n",
      "45000/45000 [==============================] - 21s 457us/sample - loss: 1.1534 - accuracy: 0.5883 - val_loss: 1.3619 - val_accuracy: 0.5352\n",
      "Epoch 29/100\n",
      "45000/45000 [==============================] - 20s 454us/sample - loss: 1.1446 - accuracy: 0.5945 - val_loss: 1.3727 - val_accuracy: 0.5316\n",
      "Epoch 30/100\n",
      "45000/45000 [==============================] - 20s 452us/sample - loss: 1.1387 - accuracy: 0.5944 - val_loss: 1.3812 - val_accuracy: 0.5238\n",
      "Epoch 31/100\n",
      "45000/45000 [==============================] - 20s 446us/sample - loss: 1.1234 - accuracy: 0.6019 - val_loss: 1.3947 - val_accuracy: 0.5240\n",
      "Epoch 32/100\n",
      "45000/45000 [==============================] - 20s 446us/sample - loss: 1.1157 - accuracy: 0.6049 - val_loss: 1.4064 - val_accuracy: 0.5238\n",
      "Epoch 33/100\n",
      "45000/45000 [==============================] - 20s 447us/sample - loss: 1.1113 - accuracy: 0.6071 - val_loss: 1.3712 - val_accuracy: 0.5326\n",
      "Epoch 34/100\n",
      "45000/45000 [==============================] - 20s 451us/sample - loss: 1.1039 - accuracy: 0.6098 - val_loss: 1.4012 - val_accuracy: 0.5224\n",
      "Epoch 35/100\n",
      "45000/45000 [==============================] - 20s 448us/sample - loss: 1.0934 - accuracy: 0.6148 - val_loss: 1.3998 - val_accuracy: 0.5300\n",
      "Epoch 36/100\n",
      "45000/45000 [==============================] - 20s 453us/sample - loss: 1.0867 - accuracy: 0.6133 - val_loss: 1.4078 - val_accuracy: 0.5230\n",
      "Epoch 37/100\n",
      "45000/45000 [==============================] - 20s 455us/sample - loss: 1.0835 - accuracy: 0.6160 - val_loss: 1.3940 - val_accuracy: 0.5344\n",
      "Epoch 38/100\n",
      "45000/45000 [==============================] - 20s 450us/sample - loss: 1.0675 - accuracy: 0.6199 - val_loss: 1.4096 - val_accuracy: 0.5316\n",
      "Epoch 39/100\n",
      "45000/45000 [==============================] - 21s 456us/sample - loss: 1.0616 - accuracy: 0.6246 - val_loss: 1.4013 - val_accuracy: 0.5264\n",
      "Epoch 40/100\n",
      "45000/45000 [==============================] - 20s 452us/sample - loss: 1.0572 - accuracy: 0.6220 - val_loss: 1.4174 - val_accuracy: 0.5292\n",
      "Epoch 41/100\n",
      "45000/45000 [==============================] - 20s 452us/sample - loss: 1.0438 - accuracy: 0.6308 - val_loss: 1.4182 - val_accuracy: 0.5236\n",
      "Epoch 42/100\n",
      "45000/45000 [==============================] - 21s 456us/sample - loss: 1.0453 - accuracy: 0.6294 - val_loss: 1.4220 - val_accuracy: 0.5250\n",
      "Epoch 43/100\n",
      "45000/45000 [==============================] - 20s 451us/sample - loss: 1.0338 - accuracy: 0.6346 - val_loss: 1.4211 - val_accuracy: 0.5218\n",
      "Epoch 44/100\n",
      "45000/45000 [==============================] - 21s 457us/sample - loss: 1.0242 - accuracy: 0.6365 - val_loss: 1.4209 - val_accuracy: 0.5230\n",
      "Epoch 45/100\n",
      "45000/45000 [==============================] - 20s 453us/sample - loss: 1.0289 - accuracy: 0.6332 - val_loss: 1.4049 - val_accuracy: 0.5346\n",
      "Epoch 46/100\n",
      "45000/45000 [==============================] - 20s 454us/sample - loss: 1.0178 - accuracy: 0.6390 - val_loss: 1.4372 - val_accuracy: 0.5258\n",
      "Epoch 47/100\n",
      "45000/45000 [==============================] - 20s 452us/sample - loss: 1.0152 - accuracy: 0.6404 - val_loss: 1.4421 - val_accuracy: 0.5280\n",
      "Epoch 48/100\n",
      "45000/45000 [==============================] - 20s 454us/sample - loss: 1.0021 - accuracy: 0.6430 - val_loss: 1.4425 - val_accuracy: 0.5216\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f4fd41824d0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=100,\n",
    "          validation_data=(X_valid, y_valid),\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 1s 143us/sample - loss: 1.3619 - accuracy: 0.5352\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.3619005823135375, 0.5352]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.load_model(\"my_cifar10_bn_model.h5\")\n",
    "model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Problem 8d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Try replacing Batch Normalization with SELU, and make the necessary adjustements to ensure the network self-normalizes (i.e., standardize the input features, use LeCun normal initialization, make sure the DNN contains only a sequence of dense layers, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=X_train.shape[1:]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100,\n",
    "                                 kernel_initializer=\"lecun_normal\",\n",
    "                                 activation=\"selu\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Nadam(lr=5e-4)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_cifar10_selu_model.h5\", save_best_only=True)\n",
    "index_run += 1\n",
    "run_logdir = os.path.join(os.curdir, \"my_cifar10_logs\", \"run_selu_{:03d}\".format(index_run))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "X_means = X_train.mean(axis=0)\n",
    "X_stds = X_train.std(axis=0)\n",
    "X_train_scaled = (X_train - X_means) / X_stds\n",
    "X_valid_scaled = (X_valid - X_means) / X_stds\n",
    "X_test_scaled = (X_test - X_means) / X_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/100\n",
      "45000/45000 [==============================] - 11s 248us/sample - loss: 1.8895 - accuracy: 0.3271 - val_loss: 1.8037 - val_accuracy: 0.3632\n",
      "Epoch 2/100\n",
      "45000/45000 [==============================] - 10s 227us/sample - loss: 1.6749 - accuracy: 0.4074 - val_loss: 1.6456 - val_accuracy: 0.4284\n",
      "Epoch 3/100\n",
      "45000/45000 [==============================] - 10s 223us/sample - loss: 1.5706 - accuracy: 0.4462 - val_loss: 1.6278 - val_accuracy: 0.4344\n",
      "Epoch 4/100\n",
      "45000/45000 [==============================] - 11s 234us/sample - loss: 1.5047 - accuracy: 0.4719 - val_loss: 1.5909 - val_accuracy: 0.4450\n",
      "Epoch 5/100\n",
      "45000/45000 [==============================] - 10s 220us/sample - loss: 1.4467 - accuracy: 0.4943 - val_loss: 1.5748 - val_accuracy: 0.4582\n",
      "Epoch 6/100\n",
      "45000/45000 [==============================] - 10s 225us/sample - loss: 1.3964 - accuracy: 0.5100 - val_loss: 1.5154 - val_accuracy: 0.4842\n",
      "Epoch 7/100\n",
      "45000/45000 [==============================] - 10s 213us/sample - loss: 1.3525 - accuracy: 0.5289 - val_loss: 1.5784 - val_accuracy: 0.4694\n",
      "Epoch 8/100\n",
      "45000/45000 [==============================] - 9s 207us/sample - loss: 1.3126 - accuracy: 0.5444 - val_loss: 1.5126 - val_accuracy: 0.4866\n",
      "Epoch 9/100\n",
      "45000/45000 [==============================] - 10s 214us/sample - loss: 1.2740 - accuracy: 0.5587 - val_loss: 1.4932 - val_accuracy: 0.4876\n",
      "Epoch 10/100\n",
      "45000/45000 [==============================] - 10s 224us/sample - loss: 1.2436 - accuracy: 0.5692 - val_loss: 1.4792 - val_accuracy: 0.4870\n",
      "Epoch 11/100\n",
      "45000/45000 [==============================] - 9s 204us/sample - loss: 1.2090 - accuracy: 0.5843 - val_loss: 1.4883 - val_accuracy: 0.4888\n",
      "Epoch 12/100\n",
      "45000/45000 [==============================] - 10s 218us/sample - loss: 1.1759 - accuracy: 0.5933 - val_loss: 1.5053 - val_accuracy: 0.5108\n",
      "Epoch 13/100\n",
      "45000/45000 [==============================] - 10s 216us/sample - loss: 1.1472 - accuracy: 0.6060 - val_loss: 1.5147 - val_accuracy: 0.5006\n",
      "Epoch 14/100\n",
      "45000/45000 [==============================] - 10s 213us/sample - loss: 1.1161 - accuracy: 0.6184 - val_loss: 1.5275 - val_accuracy: 0.5058\n",
      "Epoch 15/100\n",
      "45000/45000 [==============================] - 10s 226us/sample - loss: 1.0909 - accuracy: 0.6268 - val_loss: 1.5226 - val_accuracy: 0.4912\n",
      "Epoch 16/100\n",
      "45000/45000 [==============================] - 10s 218us/sample - loss: 1.0668 - accuracy: 0.6351 - val_loss: 1.5183 - val_accuracy: 0.5200\n",
      "Epoch 17/100\n",
      "45000/45000 [==============================] - 10s 211us/sample - loss: 1.0450 - accuracy: 0.6418 - val_loss: 1.5657 - val_accuracy: 0.5190\n",
      "Epoch 18/100\n",
      "45000/45000 [==============================] - 9s 205us/sample - loss: 1.0145 - accuracy: 0.6544 - val_loss: 1.5322 - val_accuracy: 0.5018\n",
      "Epoch 19/100\n",
      "45000/45000 [==============================] - 10s 220us/sample - loss: 0.9952 - accuracy: 0.6599 - val_loss: 1.6168 - val_accuracy: 0.5080\n",
      "Epoch 20/100\n",
      "45000/45000 [==============================] - 9s 211us/sample - loss: 0.9804 - accuracy: 0.6655 - val_loss: 1.6057 - val_accuracy: 0.5016\n",
      "Epoch 21/100\n",
      "45000/45000 [==============================] - 10s 219us/sample - loss: 0.9561 - accuracy: 0.6742 - val_loss: 1.5802 - val_accuracy: 0.4964\n",
      "Epoch 22/100\n",
      "45000/45000 [==============================] - 10s 214us/sample - loss: 0.9401 - accuracy: 0.6793 - val_loss: 1.6520 - val_accuracy: 0.4990\n",
      "Epoch 23/100\n",
      "45000/45000 [==============================] - 10s 219us/sample - loss: 0.9182 - accuracy: 0.6882 - val_loss: 1.6197 - val_accuracy: 0.5038\n",
      "Epoch 24/100\n",
      "45000/45000 [==============================] - 10s 223us/sample - loss: 0.8984 - accuracy: 0.6986 - val_loss: 1.5598 - val_accuracy: 0.5074\n",
      "Epoch 25/100\n",
      "45000/45000 [==============================] - 10s 212us/sample - loss: 0.8877 - accuracy: 0.7008 - val_loss: 1.6367 - val_accuracy: 0.5008\n",
      "Epoch 26/100\n",
      "45000/45000 [==============================] - 10s 214us/sample - loss: 0.8631 - accuracy: 0.7083 - val_loss: 1.6966 - val_accuracy: 0.5006\n",
      "Epoch 27/100\n",
      "45000/45000 [==============================] - 9s 208us/sample - loss: 0.8548 - accuracy: 0.7124 - val_loss: 1.6013 - val_accuracy: 0.5070\n",
      "Epoch 28/100\n",
      "45000/45000 [==============================] - 10s 221us/sample - loss: 0.8354 - accuracy: 0.7143 - val_loss: 1.6580 - val_accuracy: 0.4998\n",
      "Epoch 29/100\n",
      "45000/45000 [==============================] - 10s 215us/sample - loss: 0.8280 - accuracy: 0.7228 - val_loss: 1.6787 - val_accuracy: 0.5060\n",
      "Epoch 30/100\n",
      "45000/45000 [==============================] - 10s 222us/sample - loss: 0.8109 - accuracy: 0.7278 - val_loss: 1.6657 - val_accuracy: 0.5076\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f4df6924790>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_scaled, y_train, epochs=100,\n",
    "          validation_data=(X_valid_scaled, y_valid),\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 0s 75us/sample - loss: 1.4792 - accuracy: 0.4870\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.4791791608810425, 0.487]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.load_model(\"my_cifar10_selu_model.h5\")\n",
    "model.evaluate(X_valid_scaled, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Problem 8e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Try regularizing the model with alpha dropout. Then, without retraining your model, see if you can achieve better accuracy using MC Dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=X_train.shape[1:]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100,\n",
    "                                 kernel_initializer=\"lecun_normal\",\n",
    "                                 activation=\"selu\"))\n",
    "model.add(keras.layers.AlphaDropout(rate=0.1))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Nadam(lr=5e-4)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_cifar10_alpha_dropout_model.h5\", save_best_only=True)\n",
    "index_run += 1\n",
    "run_logdir = os.path.join(os.curdir, \"my_cifar10_logs\", \"run_alpha_dropout_{:03d}\".format(index_run))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/100\n",
      "45000/45000 [==============================] - 11s 239us/sample - loss: 1.8948 - accuracy: 0.3245 - val_loss: 1.7993 - val_accuracy: 0.3766\n",
      "Epoch 2/100\n",
      "45000/45000 [==============================] - 10s 212us/sample - loss: 1.6708 - accuracy: 0.4102 - val_loss: 1.6708 - val_accuracy: 0.4296\n",
      "Epoch 3/100\n",
      "45000/45000 [==============================] - 10s 230us/sample - loss: 1.5791 - accuracy: 0.4405 - val_loss: 1.6373 - val_accuracy: 0.4394\n",
      "Epoch 4/100\n",
      "45000/45000 [==============================] - 10s 212us/sample - loss: 1.5106 - accuracy: 0.4723 - val_loss: 1.6435 - val_accuracy: 0.4402\n",
      "Epoch 5/100\n",
      "45000/45000 [==============================] - 10s 217us/sample - loss: 1.4588 - accuracy: 0.4904 - val_loss: 1.5911 - val_accuracy: 0.4814\n",
      "Epoch 6/100\n",
      "45000/45000 [==============================] - 10s 233us/sample - loss: 1.4054 - accuracy: 0.5087 - val_loss: 1.5313 - val_accuracy: 0.4762\n",
      "Epoch 7/100\n",
      "45000/45000 [==============================] - 10s 218us/sample - loss: 1.3621 - accuracy: 0.5261 - val_loss: 1.6057 - val_accuracy: 0.4816\n",
      "Epoch 8/100\n",
      "45000/45000 [==============================] - 10s 218us/sample - loss: 1.3252 - accuracy: 0.5413 - val_loss: 1.5643 - val_accuracy: 0.4822\n",
      "Epoch 9/100\n",
      "45000/45000 [==============================] - 10s 219us/sample - loss: 1.2881 - accuracy: 0.5519 - val_loss: 1.5603 - val_accuracy: 0.4940\n",
      "Epoch 10/100\n",
      "45000/45000 [==============================] - 10s 213us/sample - loss: 1.2539 - accuracy: 0.5652 - val_loss: 1.5261 - val_accuracy: 0.4946\n",
      "Epoch 11/100\n",
      "45000/45000 [==============================] - 10s 217us/sample - loss: 1.2247 - accuracy: 0.5797 - val_loss: 1.5064 - val_accuracy: 0.4958\n",
      "Epoch 12/100\n",
      "45000/45000 [==============================] - 9s 210us/sample - loss: 1.1976 - accuracy: 0.5847 - val_loss: 1.6349 - val_accuracy: 0.5006\n",
      "Epoch 13/100\n",
      "45000/45000 [==============================] - 10s 218us/sample - loss: 1.1722 - accuracy: 0.5964 - val_loss: 1.5497 - val_accuracy: 0.5052\n",
      "Epoch 14/100\n",
      "45000/45000 [==============================] - 10s 216us/sample - loss: 1.1381 - accuracy: 0.6098 - val_loss: 1.6212 - val_accuracy: 0.5012\n",
      "Epoch 15/100\n",
      "45000/45000 [==============================] - 10s 217us/sample - loss: 1.1130 - accuracy: 0.6196 - val_loss: 1.6365 - val_accuracy: 0.5024\n",
      "Epoch 16/100\n",
      "45000/45000 [==============================] - 10s 212us/sample - loss: 1.0880 - accuracy: 0.6272 - val_loss: 1.6548 - val_accuracy: 0.4994\n",
      "Epoch 17/100\n",
      "45000/45000 [==============================] - 10s 212us/sample - loss: 1.0692 - accuracy: 0.6341 - val_loss: 1.6041 - val_accuracy: 0.5006\n",
      "Epoch 18/100\n",
      "45000/45000 [==============================] - 10s 212us/sample - loss: 1.0447 - accuracy: 0.6429 - val_loss: 1.6373 - val_accuracy: 0.5040\n",
      "Epoch 19/100\n",
      "45000/45000 [==============================] - 10s 230us/sample - loss: 1.0185 - accuracy: 0.6504 - val_loss: 1.6822 - val_accuracy: 0.5026\n",
      "Epoch 20/100\n",
      "45000/45000 [==============================] - 10s 213us/sample - loss: 1.0012 - accuracy: 0.6570 - val_loss: 1.7478 - val_accuracy: 0.5024\n",
      "Epoch 21/100\n",
      "45000/45000 [==============================] - 9s 204us/sample - loss: 0.9914 - accuracy: 0.6599 - val_loss: 1.7262 - val_accuracy: 0.5026\n",
      "Epoch 22/100\n",
      "45000/45000 [==============================] - 10s 221us/sample - loss: 0.9641 - accuracy: 0.6734 - val_loss: 1.6772 - val_accuracy: 0.5108\n",
      "Epoch 23/100\n",
      "45000/45000 [==============================] - 9s 202us/sample - loss: 0.9482 - accuracy: 0.6768 - val_loss: 1.7115 - val_accuracy: 0.4922\n",
      "Epoch 24/100\n",
      "45000/45000 [==============================] - 10s 216us/sample - loss: 0.9260 - accuracy: 0.6837 - val_loss: 1.6735 - val_accuracy: 0.4932\n",
      "Epoch 25/100\n",
      "45000/45000 [==============================] - 10s 215us/sample - loss: 0.9111 - accuracy: 0.6914 - val_loss: 1.8126 - val_accuracy: 0.5114\n",
      "Epoch 26/100\n",
      "45000/45000 [==============================] - 9s 208us/sample - loss: 0.8999 - accuracy: 0.6946 - val_loss: 1.8509 - val_accuracy: 0.5006\n",
      "Epoch 27/100\n",
      "45000/45000 [==============================] - 9s 205us/sample - loss: 0.8724 - accuracy: 0.7041 - val_loss: 1.7832 - val_accuracy: 0.5058\n",
      "Epoch 28/100\n",
      "45000/45000 [==============================] - 10s 213us/sample - loss: 0.8725 - accuracy: 0.7061 - val_loss: 1.8298 - val_accuracy: 0.5096\n",
      "Epoch 29/100\n",
      "45000/45000 [==============================] - 10s 230us/sample - loss: 0.8946 - accuracy: 0.6962 - val_loss: 1.8824 - val_accuracy: 0.5018\n",
      "Epoch 30/100\n",
      "45000/45000 [==============================] - 10s 215us/sample - loss: 0.8567 - accuracy: 0.7107 - val_loss: 1.7536 - val_accuracy: 0.5082\n",
      "Epoch 31/100\n",
      "45000/45000 [==============================] - 10s 218us/sample - loss: 0.8294 - accuracy: 0.7220 - val_loss: 1.8626 - val_accuracy: 0.5084\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f4df77cfc50>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_scaled, y_train, epochs=100,\n",
    "          validation_data=(X_valid_scaled, y_valid),\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 0s 79us/sample - loss: 1.5064 - accuracy: 0.4958\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.5063763664245606, 0.4958]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.load_model(\"my_cifar10_alpha_dropout_model.h5\")\n",
    "model.evaluate(X_valid_scaled, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCAlphaDropout(keras.layers.AlphaDropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_model = keras.models.Sequential([\n",
    "    MCAlphaDropout(layer.rate) if isinstance(layer, keras.layers.AlphaDropout) else layer\n",
    "    for layer in model.layers\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_dropout_predict_probas(mc_model, X, n_samples=10):\n",
    "    Y_probas = [mc_model.predict(X) for sample in range(n_samples)]\n",
    "    return np.mean(Y_probas, axis=0)\n",
    "\n",
    "def mc_dropout_predict_classes(mc_model, X, n_samples=10):\n",
    "    Y_probas = mc_dropout_predict_probas(mc_model, X, n_samples)\n",
    "    return np.argmax(Y_probas, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4988"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "y_pred = mc_dropout_predict_classes(mc_model, X_valid_scaled)\n",
    "accuracy = np.mean(y_pred == y_valid[:, 0])\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Problem 8f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Retrain your model using 1cycle scheduling and see if it improves training speed and model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=X_train.shape[1:]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100,\n",
    "                                 kernel_initializer=\"lecun_normal\",\n",
    "                                 activation=\"selu\"))\n",
    "model.add(keras.layers.AlphaDropout(rate=0.1))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(lr=1e-3)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "K = keras.backend\n",
    "\n",
    "class ExponentialLearningRate(keras.callbacks.Callback):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "        self.rates = []\n",
    "        self.losses = []\n",
    "    def on_batch_end(self, batch, logs):\n",
    "        self.rates.append(K.get_value(self.model.optimizer.lr))\n",
    "        self.losses.append(logs[\"loss\"])\n",
    "        K.set_value(self.model.optimizer.lr, self.model.optimizer.lr * self.factor)\n",
    "\n",
    "def find_learning_rate(model, X, y, epochs=1, batch_size=32, min_rate=10**-5, max_rate=10):\n",
    "    init_weights = model.get_weights()\n",
    "    iterations = len(X) // batch_size * epochs\n",
    "    factor = np.exp(np.log(max_rate / min_rate) / iterations)\n",
    "    init_lr = K.get_value(model.optimizer.lr)\n",
    "    K.set_value(model.optimizer.lr, min_rate)\n",
    "    exp_lr = ExponentialLearningRate(factor)\n",
    "    history = model.fit(X, y, epochs=epochs, batch_size=batch_size,\n",
    "                        callbacks=[exp_lr])\n",
    "    K.set_value(model.optimizer.lr, init_lr)\n",
    "    model.set_weights(init_weights)\n",
    "    return exp_lr.rates, exp_lr.losses\n",
    "\n",
    "def plot_lr_vs_loss(rates, losses):\n",
    "    plt.plot(rates, losses)\n",
    "    plt.gca().set_xscale('log')\n",
    "    plt.hlines(min(losses), min(rates), max(rates))\n",
    "    plt.axis([min(rates), max(rates), min(losses), (losses[0] + min(losses)) / 2])\n",
    "    plt.xlabel(\"Learning rate\")\n",
    "    plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45000 samples\n",
      "45000/45000 [==============================] - 1s 32us/sample - loss: nan - accuracy: 0.1390\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1e-05, 9.999868, 2.0207415, 3.5734329904828757]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "rates, losses = find_learning_rate(model, X_train_scaled, y_train, epochs=1, batch_size=batch_size)\n",
    "plot_lr_vs_loss(rates, losses)\n",
    "plt.axis([min(rates), max(rates), min(losses), (losses[0] + min(losses)) / 1.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=X_train.shape[1:]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100,\n",
    "                                 kernel_initializer=\"lecun_normal\",\n",
    "                                 activation=\"selu\"))\n",
    "\n",
    "model.add(keras.layers.AlphaDropout(rate=0.1))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(lr=best_lr)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneCycleScheduler(keras.callbacks.Callback):\n",
    "    def __init__(self, iterations, max_rate, start_rate=None,\n",
    "                 last_iterations=None, last_rate=None):\n",
    "        self.iterations = iterations\n",
    "        self.max_rate = max_rate\n",
    "        self.start_rate = start_rate or max_rate / 10\n",
    "        self.last_iterations = last_iterations or iterations // 10 + 1\n",
    "        self.half_iteration = (iterations - self.last_iterations) // 2\n",
    "        self.last_rate = last_rate or self.start_rate / 1000\n",
    "        self.iteration = 0\n",
    "    def _interpolate(self, iter1, iter2, rate1, rate2):\n",
    "        return ((rate2 - rate1) * (self.iteration - iter1)\n",
    "                / (iter2 - iter1) + rate1)\n",
    "    def on_batch_begin(self, batch, logs):\n",
    "        if self.iteration < self.half_iteration:\n",
    "            rate = self._interpolate(0, self.half_iteration, self.start_rate, self.max_rate)\n",
    "        elif self.iteration < 2 * self.half_iteration:\n",
    "            rate = self._interpolate(self.half_iteration, 2 * self.half_iteration,\n",
    "                                     self.max_rate, self.start_rate)\n",
    "        else:\n",
    "            rate = self._interpolate(2 * self.half_iteration, self.iterations,\n",
    "                                     self.start_rate, self.last_rate)\n",
    "            rate = max(rate, self.last_rate)\n",
    "        self.iteration += 1\n",
    "        K.set_value(self.model.optimizer.lr, rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/50\n",
      "45000/45000 [==============================] - 2s 40us/sample - loss: 2.0565 - accuracy: 0.2812 - val_loss: 1.7915 - val_accuracy: 0.3670\n",
      "Epoch 2/50\n",
      "45000/45000 [==============================] - 1s 29us/sample - loss: 1.7969 - accuracy: 0.3672 - val_loss: 1.7063 - val_accuracy: 0.4140\n",
      "Epoch 3/50\n",
      "45000/45000 [==============================] - 1s 29us/sample - loss: 1.6658 - accuracy: 0.4100 - val_loss: 1.6782 - val_accuracy: 0.4264\n",
      "Epoch 4/50\n",
      "45000/45000 [==============================] - 1s 29us/sample - loss: 1.5736 - accuracy: 0.4419 - val_loss: 1.6218 - val_accuracy: 0.4374\n",
      "Epoch 5/50\n",
      "45000/45000 [==============================] - 1s 28us/sample - loss: 1.5085 - accuracy: 0.4650 - val_loss: 1.6376 - val_accuracy: 0.4468\n",
      "Epoch 6/50\n",
      "45000/45000 [==============================] - 1s 29us/sample - loss: 1.4545 - accuracy: 0.4797 - val_loss: 1.5893 - val_accuracy: 0.4544\n",
      "Epoch 7/50\n",
      "45000/45000 [==============================] - 1s 29us/sample - loss: 1.4160 - accuracy: 0.4977 - val_loss: 1.6604 - val_accuracy: 0.4548\n",
      "Epoch 8/50\n",
      "45000/45000 [==============================] - 1s 29us/sample - loss: 1.3723 - accuracy: 0.5126 - val_loss: 1.5565 - val_accuracy: 0.4644\n",
      "Epoch 9/50\n",
      "45000/45000 [==============================] - 1s 28us/sample - loss: 1.3383 - accuracy: 0.5237 - val_loss: 1.5426 - val_accuracy: 0.4838\n",
      "Epoch 10/50\n",
      "45000/45000 [==============================] - 1s 29us/sample - loss: 1.3114 - accuracy: 0.5320 - val_loss: 1.5346 - val_accuracy: 0.4822\n",
      "Epoch 11/50\n",
      "45000/45000 [==============================] - 1s 29us/sample - loss: 1.2824 - accuracy: 0.5409 - val_loss: 1.6596 - val_accuracy: 0.4506\n",
      "Epoch 12/50\n",
      "45000/45000 [==============================] - 1s 29us/sample - loss: 1.2573 - accuracy: 0.5528 - val_loss: 1.6213 - val_accuracy: 0.4756\n",
      "Epoch 13/50\n",
      "45000/45000 [==============================] - 1s 29us/sample - loss: 1.2297 - accuracy: 0.5605 - val_loss: 1.6002 - val_accuracy: 0.4644\n",
      "Epoch 14/50\n",
      "45000/45000 [==============================] - 1s 29us/sample - loss: 1.2083 - accuracy: 0.5698 - val_loss: 1.6343 - val_accuracy: 0.4718\n",
      "Epoch 15/50\n",
      "45000/45000 [==============================] - 1s 29us/sample - loss: 1.1852 - accuracy: 0.5778 - val_loss: 1.6518 - val_accuracy: 0.4504\n",
      "Epoch 16/50\n",
      "45000/45000 [==============================] - 1s 29us/sample - loss: 1.1690 - accuracy: 0.5835 - val_loss: 1.6003 - val_accuracy: 0.4880\n",
      "Epoch 17/50\n",
      "45000/45000 [==============================] - 1s 29us/sample - loss: 1.1498 - accuracy: 0.5901 - val_loss: 1.7596 - val_accuracy: 0.4606\n",
      "Epoch 18/50\n",
      "45000/45000 [==============================] - 1s 29us/sample - loss: 1.1308 - accuracy: 0.5977 - val_loss: 1.6584 - val_accuracy: 0.4850\n",
      "Epoch 19/50\n",
      "45000/45000 [==============================] - 1s 29us/sample - loss: 1.1145 - accuracy: 0.6011 - val_loss: 1.6920 - val_accuracy: 0.4804\n",
      "Epoch 20/50\n",
      "45000/45000 [==============================] - 1s 29us/sample - loss: 1.0990 - accuracy: 0.6073 - val_loss: 1.6873 - val_accuracy: 0.4858\n",
      "Epoch 21/50\n",
      "45000/45000 [==============================] - 1s 29us/sample - loss: 1.0868 - accuracy: 0.6144 - val_loss: 1.6541 - val_accuracy: 0.4944\n",
      "Epoch 22/50\n",
      "45000/45000 [==============================] - 1s 29us/sample - loss: 1.0723 - accuracy: 0.6176 - val_loss: 1.6689 - val_accuracy: 0.4702\n",
      "Epoch 23/50\n",
      "45000/45000 [==============================] - 1s 29us/sample - loss: 1.0578 - accuracy: 0.6239 - val_loss: 1.6247 - val_accuracy: 0.4906\n",
      "Epoch 24/50\n",
      "45000/45000 [==============================] - 1s 29us/sample - loss: 1.0133 - accuracy: 0.6406 - val_loss: 1.7307 - val_accuracy: 0.4722\n",
      "Epoch 25/50\n",
      "45000/45000 [==============================] - 1s 29us/sample - loss: 0.9761 - accuracy: 0.6485 - val_loss: 1.7375 - val_accuracy: 0.4910\n",
      "Epoch 26/50\n",
      "45000/45000 [==============================] - 1s 29us/sample - loss: 0.9387 - accuracy: 0.6614 - val_loss: 1.7813 - val_accuracy: 0.4854\n",
      "Epoch 27/50\n",
      "45000/45000 [==============================] - 1s 29us/sample - loss: 0.9012 - accuracy: 0.6778 - val_loss: 1.7525 - val_accuracy: 0.4982\n",
      "Epoch 28/50\n",
      "45000/45000 [==============================] - 1s 29us/sample - loss: 0.8671 - accuracy: 0.6882 - val_loss: 1.8503 - val_accuracy: 0.4922\n",
      "Epoch 29/50\n",
      "45000/45000 [==============================] - 1s 29us/sample - loss: 0.8243 - accuracy: 0.7010 - val_loss: 1.8298 - val_accuracy: 0.5010\n",
      "Epoch 30/50\n",
      "45000/45000 [==============================] - 1s 29us/sample - loss: 0.7880 - accuracy: 0.7145 - val_loss: 1.8489 - val_accuracy: 0.5034\n",
      "Epoch 31/50\n",
      "45000/45000 [==============================] - 1s 29us/sample - loss: 0.7517 - accuracy: 0.7287 - val_loss: 2.1339 - val_accuracy: 0.4764\n",
      "Epoch 32/50\n",
      "45000/45000 [==============================] - 1s 29us/sample - loss: 0.7219 - accuracy: 0.7379 - val_loss: 2.1026 - val_accuracy: 0.4924\n",
      "Epoch 33/50\n",
      "45000/45000 [==============================] - 1s 29us/sample - loss: 0.6777 - accuracy: 0.7556 - val_loss: 2.0699 - val_accuracy: 0.5100\n",
      "Epoch 34/50\n",
      "45000/45000 [==============================] - 1s 29us/sample - loss: 0.6502 - accuracy: 0.7638 - val_loss: 2.1437 - val_accuracy: 0.4882\n",
      "Epoch 35/50\n",
      "45000/45000 [==============================] - 1s 29us/sample - loss: 0.6097 - accuracy: 0.7790 - val_loss: 2.2004 - val_accuracy: 0.4902\n",
      "Epoch 36/50\n",
      "45000/45000 [==============================] - 1s 29us/sample - loss: 0.5712 - accuracy: 0.7923 - val_loss: 2.3103 - val_accuracy: 0.4918\n",
      "Epoch 37/50\n",
      "45000/45000 [==============================] - 1s 29us/sample - loss: 0.5295 - accuracy: 0.8095 - val_loss: 2.2783 - val_accuracy: 0.4990\n",
      "Epoch 38/50\n",
      "45000/45000 [==============================] - 1s 29us/sample - loss: 0.4967 - accuracy: 0.8202 - val_loss: 2.4972 - val_accuracy: 0.4904\n",
      "Epoch 39/50\n",
      "45000/45000 [==============================] - 1s 29us/sample - loss: 0.4511 - accuracy: 0.8375 - val_loss: 2.4949 - val_accuracy: 0.4926\n",
      "Epoch 40/50\n",
      "45000/45000 [==============================] - 1s 29us/sample - loss: 0.4050 - accuracy: 0.8546 - val_loss: 2.6705 - val_accuracy: 0.5052\n",
      "Epoch 41/50\n",
      "45000/45000 [==============================] - 1s 29us/sample - loss: 0.3632 - accuracy: 0.8703 - val_loss: 2.7178 - val_accuracy: 0.4990\n",
      "Epoch 42/50\n",
      "45000/45000 [==============================] - 1s 29us/sample - loss: 0.3179 - accuracy: 0.8872 - val_loss: 2.8916 - val_accuracy: 0.5024\n",
      "Epoch 43/50\n",
      "45000/45000 [==============================] - 1s 29us/sample - loss: 0.2778 - accuracy: 0.9041 - val_loss: 3.1210 - val_accuracy: 0.5034\n",
      "Epoch 44/50\n",
      "45000/45000 [==============================] - 1s 29us/sample - loss: 0.2369 - accuracy: 0.9186 - val_loss: 3.2258 - val_accuracy: 0.4984\n",
      "Epoch 45/50\n",
      "45000/45000 [==============================] - 1s 29us/sample - loss: 0.2019 - accuracy: 0.9334 - val_loss: 3.4177 - val_accuracy: 0.4978\n",
      "Epoch 46/50\n",
      "45000/45000 [==============================] - 1s 29us/sample - loss: 0.1748 - accuracy: 0.9454 - val_loss: 3.5481 - val_accuracy: 0.4966\n",
      "Epoch 47/50\n",
      "45000/45000 [==============================] - 1s 29us/sample - loss: 0.1579 - accuracy: 0.9519 - val_loss: 3.6350 - val_accuracy: 0.4934\n",
      "Epoch 48/50\n",
      "45000/45000 [==============================] - 1s 29us/sample - loss: 0.1428 - accuracy: 0.9578 - val_loss: 3.7192 - val_accuracy: 0.4958\n",
      "Epoch 49/50\n",
      "45000/45000 [==============================] - 1s 29us/sample - loss: 0.1327 - accuracy: 0.9619 - val_loss: 3.7710 - val_accuracy: 0.4954\n",
      "Epoch 50/50\n",
      "45000/45000 [==============================] - 1s 29us/sample - loss: 0.1252 - accuracy: 0.9651 - val_loss: 3.7836 - val_accuracy: 0.4968\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 50\n",
    "onecycle = OneCycleScheduler(len(X_train_scaled) // batch_size * n_epochs, max_rate=0.05)\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs, batch_size=batch_size,\n",
    "                    validation_data=(X_valid_scaled, y_valid),\n",
    "                    callbacks=[onecycle])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
