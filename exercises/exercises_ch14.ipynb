{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 14: Deep Computer Vision Using Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the advantages of a CNN over a fully connected DNN for image classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Parameter efficiency: Parameters are reused over the image\n",
    "* Invariance: Patterns learned somewhere in an image are detected everywhere\n",
    "* Locality: CNNs can be constructed to combine low-level features into larger structures by chaining multiple CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a CNN composed of three convolutional layers, each with 3 × 3 kernels, a stride of 2, and \"same\" padding. The lowest layer outputs 100 feature maps, the middle one outputs 200, and the top one outputs 400. The input images are RGB images of 200 × 300 pixels.\n",
    "\n",
    "What is the total number of parameters in the CNN? If we are using 32-bit floats, at least how much RAM will this network require when making a prediction for a single instance? What about when training on a mini-batch of 50 images?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 3 input layers, 3 x 3 kernel, bias , 100 output maps -> (3 x 3 x 3 + 1) x 100 = 2800 parameters \n",
    "* 100 input layers, 3 x 3 kernel, bias, 200 output maps -> (100 x 3 x 3 + 1) x 200 = 180200 parameters\n",
    "* 200 input layers, 3 x 3 kernel, bias , 400 output maps -> (200 x 3 x 3 + 1) x 400 = 720400 parameters\n",
    "* Total parameters = 2800 + 180200 + 720400 = 903'400 parameters\n",
    "\n",
    "For inference, only two consecutive layers need to be retained in memory:\n",
    "* Same padding + stride 2: input channel 200 x 300 -> first feature map 100 x 150 -> second feature map 50 x 75 -> third feature map 25 x 38\n",
    "* 32 bits = 4 bytes\n",
    "* First layer: 4 x 100 x 100 x 150 = 6'000'000 = 6mb\n",
    "* Second layer: 4 x 200 x 50 x 75 = 3'000'000 = 3mb\n",
    "* third layer: 4 x 400 x 25 x 38 = 1'520'000 = 1.52mb\n",
    "* Assuming optimization (only two consecutive layers in memory), inference will require 9mb of RAM for the feature maps, plus 903'400 parameters ~ 3.6mb, plus the image itself, 4 * 200 * 300 x 3 pixels ~ 720kb\n",
    "\n",
    "For training, all layers need to be stored in memory for the backward pass:\n",
    "* The layers together require 6 + 3 + 1.6 = 10.5 mb\n",
    "* For a batch size of 50, 10.5 * 50 = 525mb\n",
    "* Size of images: 50 * 720kb = 36mb\n",
    "* Model parameters: 3.6 mb\n",
    "* Minimum RAM for training: 525 + 36 + 3.6 = 564.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your GPU runs out of memory while training a CNN, what are five things you could try to solve the problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Increase stride\n",
    "* Use consecutive 3x3 kernels, rather than 5x5 or 7x7 kernels\n",
    "* Smaller batch size\n",
    "* Reduce size of data to 16bits or even 8 bits\n",
    "* More maxpooling layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why would you want to add a max pooling layer rather than a convolutional layer with the same stride?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Fewer parameters to learn\n",
    "* Max pooling reinforces most dominant signal and removes noise -> acts as regularizer and improves generalization performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When would you want to add a local response normalization layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To force different feature maps to learn/explore a wider range of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you name the main innovations in AlexNet, compared to LeNet-5? What about the main innovations in GoogLeNet, ResNet, SENet, and Xception?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* AlexNet: Stack convolutional layers directly on top of one another + local response normalization\n",
    "* GoggLeNet: Inception modules\n",
    "* ResNet: Residual learning / skip connections\n",
    "* Xception: Depthwise separable convolution layer\n",
    "* SENet: SE block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is a fully convolutional network? How can you convert a dense layer into a convolutional layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A neural network composed only of convolution and pooling layers\n",
    "* Replace first dense layer with a convolution layer with kernal size equal to layer's input size, stride 1, and \"valid\" padding. The following layers need 1x1 filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fully convolutional networks are neural networks composed exclusively of convolutional and pooling layers. FCNs can efficiently process images of any width\n",
    "and height (at least above the minimum size). They are most useful for object detection and semantic segmentation because they only need to look at the image\n",
    "once (instead of having to run a CNN multiple times on different parts of the image). If you have a CNN with some dense layers on top, you can convert these\n",
    "dense layers to convolutional layers to create an FCN: just replace the lowest dense layer with a convolutional layer with a kernel size equal to the layer’s input size, with one filter per neuron in the dense layer, and using \"valid\" padding.\n",
    "Generally the stride should be 1, but you can set it to a higher value if you want. The activation function should be the same as the dense layer’s. The other dense layers should be converted the same way, but using 1 × 1 filters. It is actually possible to convert a trained CNN this way by appropriately reshaping the dense layers’ weight matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the main technical difficulty of semantic segmentation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information loss in CNNs. Pixel-level information needs to be restored for the final task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main technical difficulty of semantic segmentation is the fact that a lot of the spatial information gets lost in a CNN as the signal flows through each layer, especially in pooling layers and layers with a stride greater than 1. This spatial information needs to be restored somehow to accurately predict the class of each pixel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build your own CNN from scratch and try to achieve the highest possible accuracy on MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full,  y_train_full), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "X_valid, X_train = X_train_full[:5000]/255., X_train_full[5000:]/255.\n",
    "y_valid, y_train= y_train_full[:5000], y_train_full[5000:]\n",
    "X_test = X_test/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[..., np.newaxis].astype('float32')\n",
    "X_valid = X_valid[..., np.newaxis].astype('float32')\n",
    "X_test = X_test[..., np.newaxis].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Conv2D(64, 3, activation='relu', padding='same', input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Conv2D(64, 3, activation='relu', padding='same'),\n",
    "    keras.layers.MaxPooling2D(2),\n",
    "    keras.layers.Conv2D(128, 3, activation='relu', padding='same'),\n",
    "    keras.layers.Conv2D(128, 3, activation='relu', padding='same'),\n",
    "    keras.layers.MaxPooling2D(2),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(100, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(50, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'nadam', metrics=['accuracy', 'categorical_accuracy', 'sparse_top_k_categorical_accuracy', 'top_k_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_index = 1 # increment every time you train the model\n",
    "run_logdir = os.path.join(os.curdir, \"my_mnist_logs\", \"run_{:03d}\".format(run_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 4312), started 4:03:23 ago. (Use '!kill 4312' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-3d70e44757732dda\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-3d70e44757732dda\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir=./my_mnist_logs --port=6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelcheckpoint_cb = keras.callbacks.ModelCheckpoint('./models/my_mnist_cnn.h5', save_best_only=True)\n",
    "earlystopping_cb = keras.callbacks.EarlyStopping(patience=10)\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [modelcheckpoint_cb, earlystopping_cb, tensorboard_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "   1/1719 [..............................] - ETA: 0s - loss: 2.3051 - accuracy: 0.1562 - categorical_accuracy: 0.4688 - sparse_top_k_categorical_accuracy: 0.5312 - top_k_categorical_accuracy: 0.9062WARNING:tensorflow:From C:\\Users\\Philipp\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "   2/1719 [..............................] - ETA: 4:36 - loss: 2.2864 - accuracy: 0.1719 - categorical_accuracy: 0.2656 - sparse_top_k_categorical_accuracy: 0.5938 - top_k_categorical_accuracy: 0.8125WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0050s vs `on_train_batch_end` time: 0.3193s). Check your callbacks.\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.3878 - accuracy: 0.8791 - categorical_accuracy: 0.1012 - sparse_top_k_categorical_accuracy: 0.9804 - top_k_categorical_accuracy: 0.3939 - val_loss: 0.0621 - val_accuracy: 0.9838 - val_categorical_accuracy: 0.0962 - val_sparse_top_k_categorical_accuracy: 0.9986 - val_top_k_categorical_accuracy: 0.3202\n",
      "Epoch 2/200\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.1353 - accuracy: 0.9645 - categorical_accuracy: 0.0992 - sparse_top_k_categorical_accuracy: 0.9971 - top_k_categorical_accuracy: 0.3875 - val_loss: 0.0402 - val_accuracy: 0.9890 - val_categorical_accuracy: 0.0964 - val_sparse_top_k_categorical_accuracy: 0.9998 - val_top_k_categorical_accuracy: 0.2868\n",
      "Epoch 3/200\n",
      "1719/1719 [==============================] - 13s 8ms/step - loss: 0.1026 - accuracy: 0.9737 - categorical_accuracy: 0.0994 - sparse_top_k_categorical_accuracy: 0.9983 - top_k_categorical_accuracy: 0.3633 - val_loss: 0.0496 - val_accuracy: 0.9886 - val_categorical_accuracy: 0.0954 - val_sparse_top_k_categorical_accuracy: 1.0000 - val_top_k_categorical_accuracy: 0.2226\n",
      "Epoch 4/200\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.0811 - accuracy: 0.9795 - categorical_accuracy: 0.0994 - sparse_top_k_categorical_accuracy: 0.9989 - top_k_categorical_accuracy: 0.3546 - val_loss: 0.0271 - val_accuracy: 0.9922 - val_categorical_accuracy: 0.0964 - val_sparse_top_k_categorical_accuracy: 0.9998 - val_top_k_categorical_accuracy: 0.2434\n",
      "Epoch 5/200\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.0695 - accuracy: 0.9819 - categorical_accuracy: 0.0993 - sparse_top_k_categorical_accuracy: 0.9989 - top_k_categorical_accuracy: 0.3340 - val_loss: 0.0388 - val_accuracy: 0.9926 - val_categorical_accuracy: 0.0960 - val_sparse_top_k_categorical_accuracy: 0.9996 - val_top_k_categorical_accuracy: 0.2494\n",
      "Epoch 6/200\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.0629 - accuracy: 0.9836 - categorical_accuracy: 0.0992 - sparse_top_k_categorical_accuracy: 0.9989 - top_k_categorical_accuracy: 0.3383 - val_loss: 0.0324 - val_accuracy: 0.9928 - val_categorical_accuracy: 0.0962 - val_sparse_top_k_categorical_accuracy: 0.9996 - val_top_k_categorical_accuracy: 0.2078\n",
      "Epoch 7/200\n",
      "1719/1719 [==============================] - 13s 8ms/step - loss: 0.0558 - accuracy: 0.9848 - categorical_accuracy: 0.0993 - sparse_top_k_categorical_accuracy: 0.9992 - top_k_categorical_accuracy: 0.3194 - val_loss: 0.0306 - val_accuracy: 0.9932 - val_categorical_accuracy: 0.0964 - val_sparse_top_k_categorical_accuracy: 0.9996 - val_top_k_categorical_accuracy: 0.2048\n",
      "Epoch 8/200\n",
      "1719/1719 [==============================] - 13s 8ms/step - loss: 0.0487 - accuracy: 0.9866 - categorical_accuracy: 0.0991 - sparse_top_k_categorical_accuracy: 0.9992 - top_k_categorical_accuracy: 0.3151 - val_loss: 0.0518 - val_accuracy: 0.9912 - val_categorical_accuracy: 0.0966 - val_sparse_top_k_categorical_accuracy: 0.9998 - val_top_k_categorical_accuracy: 0.2042\n",
      "Epoch 9/200\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.0463 - accuracy: 0.9876 - categorical_accuracy: 0.0990 - sparse_top_k_categorical_accuracy: 0.9993 - top_k_categorical_accuracy: 0.3338 - val_loss: 0.0396 - val_accuracy: 0.9928 - val_categorical_accuracy: 0.0964 - val_sparse_top_k_categorical_accuracy: 0.9996 - val_top_k_categorical_accuracy: 0.1980\n",
      "Epoch 10/200\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.0412 - accuracy: 0.9885 - categorical_accuracy: 0.0989 - sparse_top_k_categorical_accuracy: 0.9995 - top_k_categorical_accuracy: 0.3230 - val_loss: 0.0487 - val_accuracy: 0.9898 - val_categorical_accuracy: 0.0958 - val_sparse_top_k_categorical_accuracy: 0.9996 - val_top_k_categorical_accuracy: 0.1986\n",
      "Epoch 11/200\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.0392 - accuracy: 0.9894 - categorical_accuracy: 0.0991 - sparse_top_k_categorical_accuracy: 0.9995 - top_k_categorical_accuracy: 0.3443 - val_loss: 0.0313 - val_accuracy: 0.9942 - val_categorical_accuracy: 0.0958 - val_sparse_top_k_categorical_accuracy: 0.9998 - val_top_k_categorical_accuracy: 0.2262\n",
      "Epoch 12/200\n",
      "1719/1719 [==============================] - 13s 8ms/step - loss: 0.0363 - accuracy: 0.9899 - categorical_accuracy: 0.0991 - sparse_top_k_categorical_accuracy: 0.9995 - top_k_categorical_accuracy: 0.3561 - val_loss: 0.0444 - val_accuracy: 0.9942 - val_categorical_accuracy: 0.0962 - val_sparse_top_k_categorical_accuracy: 0.9994 - val_top_k_categorical_accuracy: 0.2112\n",
      "Epoch 13/200\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.0374 - accuracy: 0.9903 - categorical_accuracy: 0.0993 - sparse_top_k_categorical_accuracy: 0.9993 - top_k_categorical_accuracy: 0.3362 - val_loss: 0.0324 - val_accuracy: 0.9932 - val_categorical_accuracy: 0.0962 - val_sparse_top_k_categorical_accuracy: 0.9996 - val_top_k_categorical_accuracy: 0.1918\n",
      "Epoch 14/200\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.0326 - accuracy: 0.9909 - categorical_accuracy: 0.0991 - sparse_top_k_categorical_accuracy: 0.9996 - top_k_categorical_accuracy: 0.3437 - val_loss: 0.0414 - val_accuracy: 0.9934 - val_categorical_accuracy: 0.0960 - val_sparse_top_k_categorical_accuracy: 0.9998 - val_top_k_categorical_accuracy: 0.2088\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=200, validation_data=(X_valid, y_valid), callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 0s 3ms/step - loss: 0.0414 - accuracy: 0.9934 - categorical_accuracy: 0.0960 - sparse_top_k_categorical_accuracy: 0.9998 - top_k_categorical_accuracy: 0.2088\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.041420646011829376,\n",
       " 0.993399977684021,\n",
       " 0.09600000083446503,\n",
       " 0.9998000264167786,\n",
       " 0.20880000293254852]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0045 - accuracy: 0.9987 - categorical_accuracy: 0.0991 - sparse_top_k_categorical_accuracy: 1.0000 - top_k_categorical_accuracy: 0.2107\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.004471472930163145,\n",
       " 0.9987454414367676,\n",
       " 0.09907272458076477,\n",
       " 0.9999818205833435,\n",
       " 0.21067272126674652]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 0s 3ms/step - loss: 0.0271 - accuracy: 0.0964 - categorical_accuracy: 0.0964 - sparse_top_k_categorical_accuracy: 0.9998 - top_k_categorical_accuracy: 0.2434\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.02713191509246826,\n",
       " 0.09640000015497208,\n",
       " 0.09640000015497208,\n",
       " 0.9998000264167786,\n",
       " 0.2433999925851822]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_load = keras.models.load_model('./models/my_mnist_cnn.h5')\n",
    "model_load.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0173 - accuracy: 0.0990 - categorical_accuracy: 0.0990 - sparse_top_k_categorical_accuracy: 0.9998 - top_k_categorical_accuracy: 0.2377\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.017329227179288864,\n",
       " 0.09901817888021469,\n",
       " 0.09901817888021469,\n",
       " 0.9998363852500916,\n",
       " 0.23772726953029633]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_load.evaluate(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9983636363636363"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = np.argmax(model.predict(X_train), axis=-1)\n",
    "sum(y_train == y_pred) / len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9987454545454545"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = np.argmax(model_load.predict(X_train), axis=-1)\n",
    "sum(y_train == y_pred) / len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9938"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = np.argmax(model_load.predict(X_valid), axis=-1)\n",
    "sum(y_valid == y_pred) / len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9934"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = np.argmax(model.predict(X_valid), axis=-1)\n",
    "sum(y_valid == y_pred) / len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss', 'accuracy']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_load.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss', 'accuracy']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./my_mnist_cnn_test.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test = keras.models.load_model('./my_mnist_cnn_test.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 0s 3ms/step - loss: 0.0675 - accuracy: 0.0960\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06746520102024078, 0.09600000083446503]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_test.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(model.predict(X_valid) == model_test.predict(X_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 0s 3ms/step - loss: 0.0675 - accuracy: 0.0960\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06746520102024078, 0.09600000083446503]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_test = keras.models.load_model('./my_mnist_cnn_test.h5')\n",
    "model_test.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 0s 3ms/step - loss: 0.0675 - accuracy: 0.9934\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06746520102024078, 0.993399977684021]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss', 'accuracy']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.metrics.Mean at 0x182c6d43e08>,\n",
       " <tensorflow.python.keras.metrics.MeanMetricWrapper at 0x182d3c3a248>]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.metrics.Accuracy(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.02089118, -0.03093303,  0.05925622,  0.05330097,  0.0412606 ,\n",
       "       -0.01940396, -0.07862163, -0.14383893, -0.57217515,  0.01543191,\n",
       "        0.00902614,  0.04170617, -0.34896886, -0.03886291,  0.02617732,\n",
       "        0.0256169 ,  0.24014159, -0.33885354, -0.07032701,  0.03729414,\n",
       "       -0.02416613, -0.22400498, -0.00736859, -0.13455546,  0.11253003,\n",
       "       -0.12499595,  0.10964944, -0.170484  , -0.18827076, -0.1883732 ,\n",
       "        0.03201261, -0.11146889,  0.29235902,  0.06446967, -0.06647974,\n",
       "       -0.00145599,  0.04357658, -0.00444707, -0.09551448, -0.02749488,\n",
       "        0.03206693, -0.08407733,  0.09534223, -0.14873868, -0.28262717,\n",
       "       -0.15177178,  0.07111242,  0.01771266, -0.03874651, -0.09678347,\n",
       "       -0.00453642, -0.0568822 , -0.02944077, -0.05342673, -0.01625358,\n",
       "       -0.05732775,  0.0736178 ,  0.01949573, -0.07842135, -0.0534521 ,\n",
       "        0.09091351,  0.01105258, -0.02580779, -0.18441854], dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].get_weights()[0][0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.02089118, -0.03093303,  0.05925622,  0.05330097,  0.0412606 ,\n",
       "       -0.01940396, -0.07862163, -0.14383893, -0.57217515,  0.01543191,\n",
       "        0.00902614,  0.04170617, -0.34896886, -0.03886291,  0.02617732,\n",
       "        0.0256169 ,  0.24014159, -0.33885354, -0.07032701,  0.03729414,\n",
       "       -0.02416613, -0.22400498, -0.00736859, -0.13455546,  0.11253003,\n",
       "       -0.12499595,  0.10964944, -0.170484  , -0.18827076, -0.1883732 ,\n",
       "        0.03201261, -0.11146889,  0.29235902,  0.06446967, -0.06647974,\n",
       "       -0.00145599,  0.04357658, -0.00444707, -0.09551448, -0.02749488,\n",
       "        0.03206693, -0.08407733,  0.09534223, -0.14873868, -0.28262717,\n",
       "       -0.15177178,  0.07111242,  0.01771266, -0.03874651, -0.09678347,\n",
       "       -0.00453642, -0.0568822 , -0.02944077, -0.05342673, -0.01625358,\n",
       "       -0.05732775,  0.0736178 ,  0.01949573, -0.07842135, -0.0534521 ,\n",
       "        0.09091351,  0.01105258, -0.02580779, -0.18441854], dtype=float32)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_test.layers[0].get_weights()[0][0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(model_test.layers[1].get_weights()[0][0][0][0] == model.layers[1].get_weights()[0][0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 0s 3ms/step - loss: 0.0675 - accuracy: 0.9934\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06746520102024078, 0.993399977684021]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 0s 3ms/step - loss: 0.0675 - accuracy: 0.0960\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06746520102024078, 0.09600000083446503]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_test.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 0s 3ms/step - loss: 0.0675 - accuracy: 0.0960\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06746520102024078, 0.09600000083446503]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_test.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use transfer learning for large image classification, going through these steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a training set containing at least 100 images per class. For example, you could classify your own pictures based on the location (beach, mountain, city, etc.), or alternatively you can use an existing dataset (e.g., from TensorFlow Datasets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'\n",
    "path_to_zip = tf.keras.utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)\n",
    "PATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')\n",
    "\n",
    "train_dir = os.path.join(PATH, 'train')\n",
    "validation_dir = os.path.join(PATH, 'validation')\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = (160, 160)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split it into a training set, a validation set, and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = image_dataset_from_directory(train_dir,\n",
    "                                             shuffle=True,\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             image_size=IMG_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset = image_dataset_from_directory(validation_dir,\n",
    "                                                  shuffle=True,\n",
    "                                                  batch_size=BATCH_SIZE,\n",
    "                                                  image_size=IMG_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = train_dataset.class_names\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_dataset.take(1):\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(class_names[labels[i]])\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_batches = tf.data.experimental.cardinality(validation_dataset)\n",
    "test_dataset = validation_dataset.take(val_batches // 5)\n",
    "validation_dataset = validation_dataset.skip(val_batches // 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of validation batches: %d' % tf.data.experimental.cardinality(validation_dataset))\n",
    "print('Number of test batches: %d' % tf.data.experimental.cardinality(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "validation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the input pipeline, including the appropriate preprocessing operations, and optionally add data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the base model from the pre-trained model MobileNet V2\n",
    "IMG_SHAPE = IMG_SIZE + (3,)\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_batch, label_batch = next(iter(train_dataset))\n",
    "feature_batch = base_model(image_batch)\n",
    "print(feature_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the base model architecture\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a classification head\n",
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "feature_batch_average = global_average_layer(feature_batch)\n",
    "print(feature_batch_average.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_layer = tf.keras.layers.Dense(1)\n",
    "prediction_batch = prediction_layer(feature_batch_average)\n",
    "print(prediction_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(160, 160, 3))\n",
    "x = data_augmentation(inputs)\n",
    "x = preprocess_input(x)\n",
    "x = base_model(x, training=False)\n",
    "x = global_average_layer(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "outputs = prediction_layer(x)\n",
    "model = tf.keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_learning_rate = 0.0001\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_epochs = 10\n",
    "\n",
    "loss0, accuracy0 = model.evaluate(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"initial loss: {:.2f}\".format(loss0))\n",
    "print(\"initial accuracy: {:.2f}\".format(accuracy0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_dataset,\n",
    "                    epochs=initial_epochs,\n",
    "                    validation_data=validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning curves\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.ylim([0,1.0])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tune a pretrained model on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look to see how many layers are in the base model\n",
    "print(\"Number of layers in the base model: \", len(base_model.layers))\n",
    "\n",
    "# Fine-tune from this layer onwards\n",
    "fine_tune_at = 100\n",
    "\n",
    "# Freeze all the layers before the `fine_tune_at` layer\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "    layer.trainable =  False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer = tf.keras.optimizers.RMSprop(lr=base_learning_rate/10),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tune_epochs = 10\n",
    "total_epochs =  initial_epochs + fine_tune_epochs\n",
    "\n",
    "history_fine = model.fit(train_dataset,\n",
    "                         epochs=total_epochs,\n",
    "                         initial_epoch=history.epoch[-1],\n",
    "                         validation_data=validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc += history_fine.history['accuracy']\n",
    "val_acc += history_fine.history['val_accuracy']\n",
    "\n",
    "loss += history_fine.history['loss']\n",
    "val_loss += history_fine.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.ylim([0.8, 1])\n",
    "plt.plot([initial_epochs-1,initial_epochs-1],\n",
    "          plt.ylim(), label='Start Fine Tuning')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.ylim([0, 1.0])\n",
    "plt.plot([initial_epochs-1,initial_epochs-1],\n",
    "         plt.ylim(), label='Start Fine Tuning')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(test_dataset)\n",
    "print('Test accuracy :', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieve a batch of images from the test set\n",
    "image_batch, label_batch = test_dataset.as_numpy_iterator().next()\n",
    "predictions = model.predict_on_batch(image_batch).flatten()\n",
    "\n",
    "# Apply a sigmoid since our model returns logits\n",
    "predictions = tf.nn.sigmoid(predictions)\n",
    "predictions = tf.where(predictions < 0.5, 0, 1)\n",
    "\n",
    "print('Predictions:\\n', predictions.numpy())\n",
    "print('Labels:\\n', label_batch)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(9):\n",
    "  ax = plt.subplot(3, 3, i + 1)\n",
    "  plt.imshow(image_batch[i].astype(\"uint8\"))\n",
    "  plt.title(class_names[predictions[i]])\n",
    "  plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go through TensorFlow’s Style Transfer tutorial. It is a fun way to generate art using Deep Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2 (Python 3.7.8)",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
