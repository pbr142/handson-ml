{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 11: Training Deep Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is it OK to initialize all the weights to the same value as long as that value is selected randomly using He initialization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, if all weights in a layer are the same, then gradient-based learning will update all in the same increments, i.e. the weights will always remain the same. The initial values need to be randomized to make learning possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is it OK to initialize the bias terms to 0?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, it is fine to initialize the bias term to 0. There is only one per bias per layer (per input variable)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name three advantages of the SELU activation function over ReLU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. It can take negative values, so the average value of outputs is closer to zero. This helps with the vanishing gradient problem\n",
    "2. It has a non-zero derivative.\n",
    "3. Under the right condition, it can be self-regulating (which solves the vanishing gradient problem)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In which cases would you want to use each of the following activation functions:\n",
    "SELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* SELU is a good default\n",
    "* If speed is important, leaky ReLU is preferable\n",
    "* ReLU is almost always outperformed by leaky ReLU or SELU. But, it can benefit from optimized implementation. Also, the fact that it outputs exactly zero can be ueful for optimized implementations.\n",
    "* Tanh, logistic, and softmax are almost never used for hidden layers. Only for output layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using an SGD optimizer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If momentum is large, the algorithm will pick up a lot of speed and overshoot any minimum. Too high momentum therefore slows down convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name three ways you can produce a sparse model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Set small weights to zero\n",
    "* L1-regularization\n",
    "* Tensorflow Model Optimization Toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does dropout slow down training? Does it slow down inference (i.e., making\n",
    "predictions on new instances)? What about MC Dropout?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Dropout slows down training. It has not effect on inference.\n",
    "* MC Dropout slows down training and inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Practice training a deep neural network on the CIFAR10 image dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a DNN with 20 hidden layers of 100 neurons each (that’s too many, but\n",
    "it’s the point of this exercise). Use He initialization and the ELU activation\n",
    "function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a = keras.models.Sequential([keras.layers.Flatten(input_shape=(32, 32, 3))])\n",
    "for i in range(20):\n",
    "    model_a.add(keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'))\n",
    "model_a.add(keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Nadam optimization and early stopping, train the network on the\n",
    "CIFAR10 dataset. You can load it with keras.datasets.cifar10.load_\n",
    "data(). The dataset is composed of 60,000 32 × 32–pixel color images (50,000\n",
    "for training, 10,000 for testing) with 10 classes, so you’ll need a softmax output\n",
    "layer with 10 neurons. Remember to search for the right learning rate each\n",
    "time you change the model’s architecture or hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for optimal learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = keras.backend\n",
    "class LearningRateSearcher(keras.callbacks.Callback):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "        self.learning_rate = []\n",
    "        self.loss = []\n",
    "    def on_batch_end(self, batch, logs):\n",
    "        self.learning_rate.append(K.get_value(self.model.optimizer.lr))\n",
    "        self.loss.append(logs['loss'])\n",
    "        K.set_value(self.model.optimizer.lr, self.model.optimizer.lr * self.factor)\n",
    "    def plot(self):\n",
    "        plt.plot(self.learning_rate, self.loss)\n",
    "        plt.yscale('log')\n",
    "        axes = plt.gca()\n",
    "        axes.set_ylim([min(self.loss), self.loss[0]])\n",
    "        plt.plot()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a.compile(optimizer=keras.optimizers.Nadam(learning_rate=0.001), loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_search = LearningRateSearcher(factor=1.002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 10s 6ms/step - loss: 18673150459904.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1eb90e9ee48>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_a.fit(X_train, y_train, epochs=1, callbacks=[lr_search])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr_search.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_lr = 0.00075"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate model and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a = keras.models.Sequential([keras.layers.Flatten(input_shape=(32, 32, 3))])\n",
    "for i in range(20):\n",
    "    model_a.add(keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'))\n",
    "model_a.add(keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a.compile(optimizer=keras.optimizers.Nadam(learning_rate=opt_lr), loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True)\n",
    "model_cb = keras.callbacks.ModelCheckpoint('./models/ch11/model_b.h5', save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 2.9394 - val_loss: 1.9884\n",
      "Epoch 2/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.9153 - val_loss: 1.8651\n",
      "Epoch 3/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.8603 - val_loss: 1.8179\n",
      "Epoch 4/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.8298 - val_loss: 1.7841\n",
      "Epoch 5/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.7912 - val_loss: 1.7959\n",
      "Epoch 6/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.7556 - val_loss: 1.6685\n",
      "Epoch 7/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.7182 - val_loss: 1.7227\n",
      "Epoch 8/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.7934 - val_loss: 1.6793\n",
      "Epoch 9/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.6894 - val_loss: 1.7013\n",
      "Epoch 10/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.6638 - val_loss: 1.6772\n",
      "Epoch 11/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.6381 - val_loss: 1.6367\n",
      "Epoch 12/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.6208 - val_loss: 1.6970\n",
      "Epoch 13/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.6013 - val_loss: 1.6211\n",
      "Epoch 14/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.5837 - val_loss: 1.6072\n",
      "Epoch 15/100\n",
      "1563/1563 [==============================] - 9s 5ms/step - loss: 1.7918 - val_loss: 1.7321\n",
      "Epoch 16/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.7125 - val_loss: 1.7130\n",
      "Epoch 17/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.6674 - val_loss: 1.6485\n",
      "Epoch 18/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.6385 - val_loss: 1.6320\n",
      "Epoch 19/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.6222 - val_loss: 1.6482\n",
      "Epoch 20/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.6429 - val_loss: 1.7730\n",
      "Epoch 21/100\n",
      "1563/1563 [==============================] - 9s 5ms/step - loss: 1.9078 - val_loss: 1.9351\n",
      "Epoch 22/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.8499 - val_loss: 1.8226\n",
      "Epoch 23/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.7850 - val_loss: 1.7982\n",
      "Epoch 24/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.7643 - val_loss: 1.7730\n",
      "Epoch 25/100\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.7386 - val_loss: 1.7282\n",
      "Epoch 26/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.6981 - val_loss: 1.7038\n",
      "Epoch 27/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.6700 - val_loss: 1.6541\n",
      "Epoch 28/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.6398 - val_loss: 1.6770\n",
      "Epoch 29/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.6200 - val_loss: 1.6425\n",
      "Epoch 30/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.6061 - val_loss: 1.6314\n",
      "Epoch 31/100\n",
      "1563/1563 [==============================] - 9s 5ms/step - loss: 1.5945 - val_loss: 1.7152\n",
      "Epoch 32/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.5845 - val_loss: 1.6007\n",
      "Epoch 33/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.5709 - val_loss: 1.5840\n",
      "Epoch 34/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.5676 - val_loss: 2.8326\n",
      "Epoch 35/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 4.9716 - val_loss: 1.8926\n",
      "Epoch 36/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.8064 - val_loss: 1.7881\n",
      "Epoch 37/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.7556 - val_loss: 1.7363\n",
      "Epoch 38/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.7280 - val_loss: 1.7433\n",
      "Epoch 39/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.7152 - val_loss: 1.7599\n",
      "Epoch 40/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.6967 - val_loss: 1.7192\n",
      "Epoch 41/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.6893 - val_loss: 1.6808\n",
      "Epoch 42/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.6738 - val_loss: 1.7255\n",
      "Epoch 43/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.6623 - val_loss: 1.6838\n",
      "Epoch 44/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.6466 - val_loss: 1.6570\n",
      "Epoch 45/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.6356 - val_loss: 1.6776\n",
      "Epoch 46/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.6294 - val_loss: 1.6516\n",
      "Epoch 47/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.6074 - val_loss: 1.6297\n",
      "Epoch 48/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.6005 - val_loss: 1.6177\n",
      "Epoch 49/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.5917 - val_loss: 1.6975\n",
      "Epoch 50/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.5881 - val_loss: 1.5831\n",
      "Epoch 51/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.6997 - val_loss: 1.6791\n",
      "Epoch 52/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.6381 - val_loss: 1.6543\n",
      "Epoch 53/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 47.4033 - val_loss: 1.7416\n",
      "Epoch 54/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.7654 - val_loss: 1.7809\n",
      "Epoch 55/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.7109 - val_loss: 1.7033\n",
      "Epoch 56/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.6658 - val_loss: 1.6649\n",
      "Epoch 57/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.6483 - val_loss: 1.7060\n",
      "Epoch 58/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.6338 - val_loss: 1.6388\n",
      "Epoch 59/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.6232 - val_loss: 1.6234\n",
      "Epoch 60/100\n",
      "1563/1563 [==============================] - 9s 5ms/step - loss: 1.6119 - val_loss: 1.6180\n",
      "Epoch 61/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.6031 - val_loss: 1.6434\n",
      "Epoch 62/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.5930 - val_loss: 1.6463\n",
      "Epoch 63/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.5906 - val_loss: 1.6118\n",
      "Epoch 64/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 4.3960 - val_loss: 1.9569\n",
      "Epoch 65/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 7.0119 - val_loss: 1.8375\n",
      "Epoch 66/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.8049 - val_loss: 1.7955\n",
      "Epoch 67/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.7746 - val_loss: 1.7589\n",
      "Epoch 68/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.7564 - val_loss: 1.7511\n",
      "Epoch 69/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.7363 - val_loss: 1.7518\n",
      "Epoch 70/100\n",
      "1563/1563 [==============================] - 9s 5ms/step - loss: 1.7215 - val_loss: 1.7444\n"
     ]
    }
   ],
   "source": [
    "history_a = model_a.fit(X_train, y_train, epochs=100, callbacks=[early_stopping_cb, model_cb], validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_b = keras.models.load_model('./models/ch11/model_b.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step - loss: 1.5831\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.5831236839294434"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_b.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Now try adding Batch Normalization and compare the learning curves: Is it\n",
    "converging faster than before? Does it produce a better model? How does it\n",
    "affect training speed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Try replacing Batch Normalization with SELU, and make the necessary adjustements\n",
    "to ensure the network self-normalizes (i.e., standardize the input features,\n",
    "use LeCun normal initialization, make sure the DNN contains only a\n",
    "sequence of dense layers, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Try regularizing the model with alpha dropout. Then, without retraining your\n",
    "model, see if you can achieve better accuracy using MC Dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrain your model using 1cycle scheduling and see if it improves training speed and model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2 (Python 3.7.8)",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
